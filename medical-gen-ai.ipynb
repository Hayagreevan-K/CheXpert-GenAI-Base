{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2169393,"sourceType":"datasetVersion","datasetId":1302315},{"sourceId":13662577,"sourceType":"datasetVersion","datasetId":8686533}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---- Correct Kaggle installation cell ----\n# Make sure we have all required packages for this notebook.\n\n# Torch & torchvision are already included in Kaggle, so we skip reinstalling them.\n\n!pip install --quiet timm scikit-learn pandas pillow tqdm\n# pytorch-grad-cam (install latest compatible version)\n!pip install --quiet git+https://github.com/jacobgil/pytorch-grad-cam.git\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:37:48.221049Z","iopub.execute_input":"2025-11-09T04:37:48.221298Z","iopub.status.idle":"2025-11-09T04:39:19.966426Z","shell.execute_reply.started":"2025-11-09T04:37:48.221274Z","shell.execute_reply":"2025-11-09T04:39:19.965622Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\ndatasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.2.6 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Cell 2: 01_safe_csv_cleaning\nimport os, csv, numpy as np, sys\nBASE = \"/kaggle/input/chexpert\"\nSRC_CSV = os.path.join(BASE, \"train.csv\")\nCLEAN_CSV = \"/kaggle/working/chexpert_train_clean.csv\"\nOUT_RESOLVED = \"/kaggle/working/chexpert_train_resolved.csv\"\n\nassert os.path.exists(SRC_CSV), f\"Source CSV not found: {SRC_CSV}\"\n\n# 1) Write cleaned CSV (coerce all cells to simple strings)\nwith open(SRC_CSV, 'r', errors='replace') as fr, open(CLEAN_CSV, 'w', newline='') as fw:\n    reader = csv.reader(fr)\n    writer = csv.writer(fw)\n    try:\n        header = next(reader)\n    except StopIteration:\n        raise RuntimeError(\"Source CSV appears empty.\")\n    header = [str(h).strip() for h in header]\n    writer.writerow(header)\n    row_count = 0\n    for r in reader:\n        if len(r) < len(header):\n            r = r + [''] * (len(header) - len(r))\n        elif len(r) > len(header):\n            r = r[:len(header)-1] + [\",\".join(r[len(header)-1:])]\n        clean_row = []\n        for cell in r:\n            if cell is None:\n                clean_row.append(\"\")\n            elif isinstance(cell, (list, tuple, np.ndarray)):\n                clean_row.append(\",\".join(map(str, cell)))\n            else:\n                clean_row.append(str(cell))\n        writer.writerow(clean_row)\n        row_count += 1\nprint(f\"Wrote cleaned CSV rows: {row_count} -> {CLEAN_CSV}\")\n\n# 2) Read cleaned CSV with csv.reader and resolve paths into absolute Kaggle paths\nwith open(CLEAN_CSV, 'r', newline='') as fr:\n    reader = csv.reader(fr)\n    header = next(reader)\n    header = [h.strip() for h in header]\n    records = []\n    for r in reader:\n        if len(r) < len(header):\n            r = r + [''] * (len(header) - len(r))\n        elif len(r) > len(header):\n            r = r[:len(header)-1] + [\",\".join(r[len(header)-1:])]\n        rec = {h: (r[i].strip() if r[i] != '' else None) for i,h in enumerate(header)}\n        records.append(rec)\nprint(\"Parsed cleaned CSV rows:\", len(records))\nimg_col = None\nfor c in header:\n    if c.lower() in (\"path\",\"image\",\"image_path\",\"filename\"):\n        img_col = c; break\nif img_col is None:\n    img_col = header[0]\nprint(\"Detected image column:\", img_col)\n\ndef normalize_path(s):\n    if s is None:\n        return None\n    s = str(s).strip()\n    if s == \"\":\n        return None\n    b = os.path.basename(s)\n    if b.startswith(\"._\"):\n        return None\n    if os.path.isabs(s) and os.path.exists(s):\n        return s\n    for marker in (\"train/\", \"valid/\"):\n        idx = s.find(marker)\n        if idx != -1:\n            candidate = os.path.join(BASE, s[idx:])\n            if os.path.exists(candidate):\n                return candidate\n    prefix = \"CheXpert-v1.0-small/\"\n    if s.startswith(prefix):\n        cand = os.path.join(BASE, s[len(prefix):])\n        if os.path.exists(cand):\n            return cand\n    cand2 = os.path.join(BASE, s)\n    if os.path.exists(cand2):\n        return cand2\n    # search fallback\n    for root_sub in (\"train\",\"valid\"):\n        for root, dirs, files in os.walk(os.path.join(BASE, root_sub)):\n            if b in files:\n                return os.path.join(root, b)\n    return None\n\nresolved_count = 0\nresolved_examples = []\nunresolved_examples = []\nfor rec in records:\n    raw = rec.get(img_col)\n    rp = normalize_path(raw)\n    rec['resolved_path'] = rp\n    if rp is not None:\n        resolved_count += 1\n        if len(resolved_examples) < 8:\n            resolved_examples.append(rp)\n    else:\n        if len(unresolved_examples) < 8:\n            unresolved_examples.append(raw)\n\ntotal = len(records)\nprint(f\"Resolved paths: {resolved_count} / {total}\")\n\n# write final resolved CSV\nout_header = header + [\"resolved_path\"]\nwith open(OUT_RESOLVED, 'w', newline='') as fw:\n    writer = csv.writer(fw)\n    writer.writerow(out_header)\n    for rec in records:\n        row = [rec.get(h, \"\") if rec.get(h, None) is not None else \"\" for h in header]\n        row.append(rec.get('resolved_path') or \"\")\n        writer.writerow(row)\nprint(\"Saved final resolved CSV to:\", OUT_RESOLVED)\n\n# Print some samples\nprint(\"\\nSample resolved paths:\")\nfor p in resolved_examples[:8]:\n    print(\" -\", p)\nif unresolved_examples:\n    print(\"\\nSample unresolved (if any):\")\n    for s in unresolved_examples[:8]:\n        print(\" -\", s)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:39:30.779607Z","iopub.execute_input":"2025-11-09T04:39:30.779895Z","iopub.status.idle":"2025-11-09T05:02:59.645737Z","shell.execute_reply.started":"2025-11-09T04:39:30.779863Z","shell.execute_reply":"2025-11-09T05:02:59.644903Z"}},"outputs":[{"name":"stdout","text":"Wrote cleaned CSV rows: 223414 -> /kaggle/working/chexpert_train_clean.csv\nParsed cleaned CSV rows: 223414\nDetected image column: Path\nResolved paths: 223414 / 223414\nSaved final resolved CSV to: /kaggle/working/chexpert_train_resolved.csv\n\nSample resolved paths:\n - /kaggle/input/chexpert/train/patient00001/study1/view1_frontal.jpg\n - /kaggle/input/chexpert/train/patient00002/study2/view1_frontal.jpg\n - /kaggle/input/chexpert/train/patient00002/study1/view1_frontal.jpg\n - /kaggle/input/chexpert/train/patient00002/study1/view2_lateral.jpg\n - /kaggle/input/chexpert/train/patient00003/study1/view1_frontal.jpg\n - /kaggle/input/chexpert/train/patient00004/study1/view1_frontal.jpg\n - /kaggle/input/chexpert/train/patient00004/study1/view2_lateral.jpg\n - /kaggle/input/chexpert/train/patient00005/study1/view1_frontal.jpg\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 3: 02_configs\nWORKING_RESOLVED_CSV = \"/kaggle/working/chexpert_train_resolved.csv\"  # created above\nassert os.path.exists(WORKING_RESOLVED_CSV), \"Resolved CSV missing; run Cell 2.\"\n\n# Label list from CheXpert CSV header (edit if you prefer a subset)\nLABELS = [\"No Finding\",\"Enlarged Cardiomediastinum\",\"Cardiomegaly\",\"Lung Opacity\",\n          \"Lung Lesion\",\"Edema\",\"Consolidation\",\"Pneumonia\",\"Atelectasis\",\n          \"Pneumothorax\",\"Pleural Effusion\",\"Pleural Other\",\"Fracture\",\"Support Devices\"]\n\nIMAGE_COL = \"resolved_path\"\nIMAGE_SIZE = 320\nBATCH_SIZE = 16\nNUM_WORKERS = 2\nLR = 1e-4\nEPOCHS = 2      # small by default for quick runs\nDEVICE = \"cuda\"\n\n# Continual learning params\nN_TASKS = 4\nEXEMPLAR_BUDGET = 1000\nEXEMPLARS_PER_BATCH = 8\nUSE_EWC = True\nEWC_LAMBDA = 1000.0\nFISHER_SAMPLES = 200\n\nprint(\"Config OK. Resolved CSV:\", WORKING_RESOLVED_CSV)\nprint(\"Labels len:\", len(LABELS))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T05:04:29.491899Z","iopub.execute_input":"2025-11-09T05:04:29.492495Z","iopub.status.idle":"2025-11-09T05:04:29.498352Z","shell.execute_reply.started":"2025-11-09T05:04:29.492471Z","shell.execute_reply":"2025-11-09T05:04:29.497541Z"}},"outputs":[{"name":"stdout","text":"Config OK. Resolved CSV: /kaggle/working/chexpert_train_resolved.csv\nLabels len: 14\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Cell 4: 03_imports_and_device\nimport math, random, time\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nimport numpy as np\nimport pandas as pd\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() and DEVICE == \"cuda\" else \"cpu\")\nprint(\"Device:\", DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T05:04:32.843717Z","iopub.execute_input":"2025-11-09T05:04:32.844510Z","iopub.status.idle":"2025-11-09T05:04:39.532069Z","shell.execute_reply.started":"2025-11-09T05:04:32.844485Z","shell.execute_reply":"2025-11-09T05:04:39.531398Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Cell 5_repl: 04_load_and_split_no_pandas\n# Load resolved CSV using csv.reader into pure-Python records, then create train/val splits.\nimport csv, random, math, os\nfrom pprint import pprint\n\nRESOLVED_CSV = \"/kaggle/working/chexpert_train_resolved.csv\"\nassert os.path.exists(RESOLVED_CSV), f\"Resolved CSV not found: {RESOLVED_CSV}\"\n\n# Read with csv.reader\nwith open(RESOLVED_CSV, 'r', newline='') as fr:\n    reader = csv.reader(fr)\n    header = next(reader)\n    header = [h.strip() for h in header]\n    records = []\n    for r in reader:\n        if len(r) < len(header):\n            r = r + [''] * (len(header) - len(r))\n        elif len(r) > len(header):\n            r = r[:len(header)-1] + [\",\".join(r[len(header)-1:])]\n        rec = {h: (r[i].strip() if r[i] != '' else None) for i,h in enumerate(header)}\n        records.append(rec)\n\nprint(\"Loaded resolved CSV rows:\", len(records))\nprint(\"Columns sample (first 12):\", header[:12])\n# Show sample record keys\npprint(records[0], depth=1)\n\n# Ensure LABELS exist; coerce label strings to floats or None\nfor rec in records:\n    for lab in LABELS:\n        v = rec.get(lab, None)\n        if v is None or v == \"\":\n            rec[lab] = None\n        else:\n            try:\n                rec[lab] = float(v)\n            except:\n                # fallback: try removing stray characters\n                try:\n                    rec[lab] = float(v.replace(',', '').strip())\n                except:\n                    rec[lab] = None\n\n# Shuffle & split indices into train/val (same split ratio as before: 88% train)\nrandom.seed(SEED)\nindices = list(range(len(records)))\nrandom.shuffle(indices)\nsplit = int(len(indices) * 0.88)\ntrain_idx = indices[:split]\nval_idx = indices[split:]\ntrain_records = [records[i] for i in train_idx]\nval_records = [records[i] for i in val_idx]\n\nprint(\"Train records:\", len(train_records), \"Val records:\", len(val_records))\n# quick sanity show\nprint(\"Sample train resolved_path:\", train_records[0].get(IMAGE_COL))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T05:04:52.308209Z","iopub.execute_input":"2025-11-09T05:04:52.309111Z","iopub.status.idle":"2025-11-09T05:04:54.575264Z","shell.execute_reply.started":"2025-11-09T05:04:52.309085Z","shell.execute_reply":"2025-11-09T05:04:54.574638Z"}},"outputs":[{"name":"stdout","text":"Loaded resolved CSV rows: 223414\nColumns sample (first 12): ['Path', 'Sex', 'Age', 'Frontal/Lateral', 'AP/PA', 'No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', 'Lung Lesion', 'Edema', 'Consolidation']\n{'AP/PA': 'AP',\n 'Age': '68',\n 'Atelectasis': None,\n 'Cardiomegaly': None,\n 'Consolidation': None,\n 'Edema': None,\n 'Enlarged Cardiomediastinum': None,\n 'Fracture': None,\n 'Frontal/Lateral': 'Frontal',\n 'Lung Lesion': None,\n 'Lung Opacity': None,\n 'No Finding': '1.0',\n 'Path': 'CheXpert-v1.0-small/train/patient00001/study1/view1_frontal.jpg',\n 'Pleural Effusion': None,\n 'Pleural Other': None,\n 'Pneumonia': None,\n 'Pneumothorax': '0.0',\n 'Sex': 'Female',\n 'Support Devices': '1.0',\n 'resolved_path': '/kaggle/input/chexpert/train/patient00001/study1/view1_frontal.jpg'}\nTrain records: 196604 Val records: 26810\nSample train resolved_path: /kaggle/input/chexpert/train/patient13747/study12/view1_frontal.jpg\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# REPLACEMENT: dataset cell that avoids torch.from_numpy completely\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom PIL import Image\n\ndef pil_to_tensor_safe(pil_img, image_size=IMAGE_SIZE):\n    # Resize\n    if pil_img.mode != \"RGB\":\n        pil_img = pil_img.convert(\"RGB\")\n    pil_img = pil_img.resize((image_size, image_size), resample=Image.BILINEAR)\n    arr = np.asarray(pil_img, dtype=np.uint8)\n    if arr.ndim == 2:\n        arr = np.stack([arr]*3, axis=-1)\n    arr = arr.astype(np.float32) / 255.0\n    mean = np.array([0.485, 0.485, 0.485], dtype=np.float32)\n    std = np.array([0.229, 0.229, 0.229], dtype=np.float32)\n    arr = (arr - mean) / std\n    arr = np.transpose(arr, (2,0,1)).copy()   # CHW\n    # Convert to plain nested lists to avoid numpy->torch ufunc mismatch\n    arr_list = arr.tolist()\n    tensor = torch.tensor(arr_list, dtype=torch.float32)\n    return tensor\n\nclass CXRRecordsDatasetSafe(Dataset):\n    def __init__(self, records, image_col=IMAGE_COL, label_cols=LABELS, image_size=IMAGE_SIZE):\n        self.records = records\n        self.image_col = image_col\n        self.label_cols = label_cols\n        self.image_size = image_size\n\n    def __len__(self):\n        return len(self.records)\n\n    def __getitem__(self, idx):\n        rec = self.records[idx]\n        path = rec.get(self.image_col)\n        if path is None:\n            raise RuntimeError(f\"Record {idx} missing image path.\")\n        with Image.open(path) as img:\n            img = img.convert(\"RGB\")\n            tensor = pil_to_tensor_safe(img, image_size=self.image_size)\n        labels = []\n        mask = []\n        for c in self.label_cols:\n            v = rec.get(c, None)\n            if v is None or (isinstance(v, float) and np.isnan(v)):\n                labels.append(0.0); mask.append(0.0)\n            else:\n                labels.append(float(v)); mask.append(1.0)\n        labels = torch.tensor(labels, dtype=torch.float32)\n        mask = torch.tensor(mask, dtype=torch.float32)\n        return tensor, labels, mask, path\n\n# Quick loader test\nsample_recs = train_records[:32]\nds = CXRRecordsDatasetSafe(sample_recs)\nloader = DataLoader(ds, batch_size=4, shuffle=False, num_workers=0)\nfor imgs, labs, masks, paths in loader:\n    print(\"Batch imgs\", imgs.shape, \"labs\", labs.shape)\n    break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T05:05:16.352298Z","iopub.execute_input":"2025-11-09T05:05:16.353004Z","iopub.status.idle":"2025-11-09T05:05:16.626736Z","shell.execute_reply.started":"2025-11-09T05:05:16.352966Z","shell.execute_reply":"2025-11-09T05:05:16.626116Z"}},"outputs":[{"name":"stdout","text":"Batch imgs torch.Size([4, 3, 320, 320]) labs torch.Size([4, 14])\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Cell 7_repl: 06_model_utils  (unchanged)\ndef get_densenet(num_labels=len(LABELS), pretrained=True):\n    model = models.densenet121(pretrained=pretrained)\n    in_features = model.classifier.in_features\n    model.classifier = nn.Linear(in_features, num_labels)\n    return model\n\ndef expand_classifier_head(model, new_num_labels):\n    old = model.classifier\n    old_out = old.out_features\n    in_f = old.in_features\n    new_layer = nn.Linear(in_f, new_num_labels)\n    with torch.no_grad():\n        new_layer.weight[:old_out, :] = old.weight\n        new_layer.bias[:old_out] = old.bias\n    model.classifier = new_layer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T05:05:19.932002Z","iopub.execute_input":"2025-11-09T05:05:19.932493Z","iopub.status.idle":"2025-11-09T05:05:19.937222Z","shell.execute_reply.started":"2025-11-09T05:05:19.932470Z","shell.execute_reply":"2025-11-09T05:05:19.936533Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Cell 8_repl: 07_exemplar_buffer  (unchanged)\nclass ExemplarBuffer:\n    def __init__(self, budget:int):\n        self.budget = budget\n        self.paths = []\n        self.labels = []\n\n    def __len__(self):\n        return len(self.paths)\n\n    def add(self, paths:list, labels:list):\n        for p, l in zip(paths, labels):\n            self.paths.append(p)\n            self.labels.append(np.array(l, dtype=np.float32))\n        self._prune()\n\n    def _prune(self):\n        while len(self.paths) > self.budget:\n            self.paths.pop(0)\n            self.labels.pop(0)\n\n    def sample(self, k:int):\n        if len(self.paths) == 0:\n            return [], []\n        k = min(k, len(self.paths))\n        idx = np.random.choice(len(self.paths), k, replace=False)\n        return [self.paths[i] for i in idx], [self.labels[i] for i in idx]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T05:05:25.101021Z","iopub.execute_input":"2025-11-09T05:05:25.101690Z","iopub.status.idle":"2025-11-09T05:05:25.110599Z","shell.execute_reply.started":"2025-11-09T05:05:25.101655Z","shell.execute_reply":"2025-11-09T05:05:25.109700Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Cell 9_repl: 08_ewc  (unchanged)\ndef compute_fisher(model, dataloader, device, samples=200):\n    model.eval()\n    fisher = {}\n    for n,p in model.named_parameters():\n        if p.requires_grad:\n            fisher[n] = torch.zeros_like(p.data).to(device)\n    cnt = 0\n    for imgs, labels, masks, _ in dataloader:\n        imgs = imgs.to(device); labels = labels.to(device)\n        model.zero_grad()\n        outputs = model(imgs)\n        loss = F.binary_cross_entropy_with_logits(outputs, labels, reduction='sum')\n        loss.backward()\n        for n,p in model.named_parameters():\n            if p.requires_grad and p.grad is not None:\n                fisher[n] += (p.grad.data ** 2)\n        cnt += imgs.size(0)\n        if cnt >= samples:\n            break\n    for n in fisher:\n        fisher[n] /= max(1.0, float(cnt))\n    return fisher\n\ndef ewc_penalty(model, fisher, old_params, lambda_ewc):\n    loss = 0.0\n    for n,p in model.named_parameters():\n        if n in fisher:\n            loss = loss + (fisher[n] * (p - old_params[n]).pow(2)).sum()\n    return lambda_ewc * loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T05:05:34.036368Z","iopub.execute_input":"2025-11-09T05:05:34.037137Z","iopub.status.idle":"2025-11-09T05:05:34.043472Z","shell.execute_reply.started":"2025-11-09T05:05:34.037107Z","shell.execute_reply":"2025-11-09T05:05:34.042727Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Cell 10_repl: 09_training_utils_no_pandas\ndef train_one_epoch(model, optimizer, dataloader, exemplar_buffer, device, ewc_fisher=None, old_params=None):\n    model.train()\n    total_loss = 0.0\n    for imgs, labels, masks, _ in tqdm(dataloader, desc=\"Train\"):\n        imgs = imgs.to(device); labels = labels.to(device); masks = masks.to(device)\n        ex_paths, ex_labels = exemplar_buffer.sample(EXEMPLARS_PER_BATCH)\n        if len(ex_paths) > 0:\n            ex_tensors = []\n            ex_lbls = []\n            for p, lab in zip(ex_paths, ex_labels):\n                img_p = Image.open(p).convert(\"RGB\")\n                img_p = transforms.Resize((IMAGE_SIZE, IMAGE_SIZE))(img_p)\n                img_p = transforms.ToTensor()(img_p)\n                img_p = transforms.Normalize([0.485,0.485,0.485],[0.229,0.229,0.229])(img_p)\n                ex_tensors.append(img_p)\n                ex_lbls.append(torch.from_numpy(lab))\n            ex_tensors = torch.stack(ex_tensors).to(device)\n            ex_lbls = torch.stack(ex_lbls).to(device)\n            imgs = torch.cat([imgs, ex_tensors], dim=0)\n            labels = torch.cat([labels, ex_lbls], dim=0)\n            masks = torch.cat([masks, torch.ones_like(ex_lbls)], dim=0)\n\n        optimizer.zero_grad()\n        outputs = model(imgs)\n        bce = F.binary_cross_entropy_with_logits(outputs, labels, reduction='none')\n        bce = (bce * masks).sum() / (masks.sum().clamp_min(1.0))\n        loss = bce\n        if ewc_fisher is not None and old_params is not None:\n            loss = loss + ewc_penalty(model, ewc_fisher, old_params, EWC_LAMBDA)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(dataloader)\n\n# pure-numpy AUC (no sklearn)\ndef _auc_from_ranks(y_true, y_score):\n    y_true = np.asarray(y_true)\n    y_score = np.asarray(y_score)\n    n_pos = int(np.sum(y_true == 1))\n    n_neg = int(np.sum(y_true == 0))\n    if n_pos == 0 or n_neg == 0:\n        return float('nan')\n    order = np.argsort(y_score)\n    ranks = np.empty_like(order, dtype=float)\n    sorted_scores = y_score[order]\n    i = 0; rank = 1; N = len(y_score)\n    while i < N:\n        j = i+1\n        while j < N and sorted_scores[j] == sorted_scores[i]:\n            j += 1\n        avg_rank = (rank + (rank + (j - i) - 1)) / 2.0\n        ranks[i:j] = avg_rank\n        rank += (j - i)\n        i = j\n    inv_ranks = np.empty_like(ranks)\n    inv_ranks[order] = ranks\n    sum_ranks_pos = np.sum(inv_ranks[y_true == 1])\n    auc = (sum_ranks_pos - n_pos * (n_pos + 1) / 2.0) / (n_pos * n_neg)\n    return float(auc)\n\ndef evaluate_auroc(model, dataloader, device, label_cols):\n    model.eval()\n    preds_list = []; targets_list = []\n    with torch.no_grad():\n        for imgs, labels, masks, _ in tqdm(dataloader, desc=\"Eval\"):\n            imgs = imgs.to(device)\n            out = torch.sigmoid(model(imgs)).detach().cpu().numpy()\n            preds_list.append(out)\n            targets_list.append(labels.numpy())\n    if len(preds_list) == 0:\n        return {lab: float('nan') for lab in label_cols}, float('nan')\n    preds = np.vstack(preds_list)\n    targets = np.vstack(targets_list)\n    aucs = {}\n    for i, lab in enumerate(label_cols):\n        y_true = targets[:, i]\n        if len(np.unique(y_true[~np.isnan(y_true)])) < 2:\n            aucs[lab] = float('nan'); continue\n        y_bin = (y_true >= 0.5).astype(int)\n        s = preds[:, i]\n        try:\n            a = _auc_from_ranks(y_bin, s)\n            aucs[lab] = a\n        except:\n            aucs[lab] = float('nan')\n    vals = [v for v in aucs.values() if not np.isnan(v)]\n    mean_auc = float(np.mean(vals)) if len(vals) > 0 else float('nan')\n    return aucs, mean_auc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T05:05:39.574125Z","iopub.execute_input":"2025-11-09T05:05:39.574380Z","iopub.status.idle":"2025-11-09T05:05:39.588393Z","shell.execute_reply.started":"2025-11-09T05:05:39.574362Z","shell.execute_reply":"2025-11-09T05:05:39.587551Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Cell 11_repl: 10_gradcam_and_report  (unchanged)\ndef make_gradcam_visual(model, pil_img, device):\n    try:\n        from pytorch_grad_cam import GradCAM\n        from pytorch_grad_cam.utils.image import show_cam_on_image\n    except Exception as e:\n        print(\"pytorch-grad-cam import failed:\", e)\n        return None\n    model.eval()\n    transform = transforms.Compose([\n        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485,0.485,0.485],[0.229,0.229,0.229])\n    ])\n    inp = transform(pil_img).unsqueeze(0).to(device)\n    target_layers = [model.features.denseblock4] if hasattr(model, 'features') else [list(model.children())[-2]]\n    cam = GradCAM(model=model, target_layers=target_layers, use_cuda=(device.type=='cuda'))\n    grayscale_cam = cam(input_tensor=inp, targets=None)\n    np_img = np.array(pil_img.resize((IMAGE_SIZE, IMAGE_SIZE))).astype(np.float32)/255.0\n    vis = show_cam_on_image(np_img, grayscale_cam[0], use_rgb=True)\n    from PIL import Image as PILImage\n    return PILImage.fromarray(vis)\n\ndef build_report_from_preds(preds: dict):\n    findings = []\n    for k,v in preds.items():\n        if v > 0.20:\n            findings.append(f\"- {k}: approx {int(round(v*100))}%\")\n    if not findings:\n        findings = [\"No acute cardiopulmonary disease detected with high probability.\"]\n    prompt = f\"\"\"You are a radiology assistant. Produce a concise chest X-ray report with sections: Exam, Findings, Impression.\nExam: Chest radiograph (frontal). Findings:\n{chr(10).join(findings)}\n\nImpression:\n- Summarize the findings above using cautious language and recommend radiologist review.\n\"\"\"\n    return prompt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T05:05:45.198801Z","iopub.execute_input":"2025-11-09T05:05:45.199088Z","iopub.status.idle":"2025-11-09T05:05:45.206131Z","shell.execute_reply.started":"2025-11-09T05:05:45.199066Z","shell.execute_reply":"2025-11-09T05:05:45.205476Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# REPLACE dataset with safe version and helper functions\nimport numpy as np\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\n\n# Safe PIL -> torch.tensor conversion that avoids torch.from_numpy path\ndef pil_to_tensor_safe(pil_img, image_size=IMAGE_SIZE):\n    if pil_img.mode != \"RGB\":\n        pil_img = pil_img.convert(\"RGB\")\n    pil_img = pil_img.resize((image_size, image_size), resample=Image.BILINEAR)\n    arr = np.asarray(pil_img, dtype=np.uint8)\n    if arr.ndim == 2:\n        arr = np.stack([arr]*3, axis=-1)\n    arr = arr.astype(np.float32) / 255.0\n    mean = np.array([0.485, 0.485, 0.485], dtype=np.float32)\n    std  = np.array([0.229, 0.229, 0.229], dtype=np.float32)\n    arr = (arr - mean) / std\n    arr = np.transpose(arr, (2,0,1)).copy()   # CHW\n    # Convert to plain nested lists then to torch.tensor (robust)\n    arr_list = arr.tolist()\n    tensor = torch.tensor(arr_list, dtype=torch.float32)\n    return tensor\n\n# Safe dataset that returns tensors prepared with pil_to_tensor_safe\nclass CXRRecordsDatasetSafe(Dataset):\n    def __init__(self, records, image_col=IMAGE_COL, label_cols=LABELS, image_size=IMAGE_SIZE):\n        self.records = records\n        self.image_col = image_col\n        self.label_cols = label_cols\n        self.image_size = image_size\n\n    def __len__(self):\n        return len(self.records)\n\n    def __getitem__(self, idx):\n        rec = self.records[idx]\n        path = rec.get(self.image_col)\n        if path is None:\n            raise RuntimeError(f\"Record {idx} missing image path.\")\n        # open and convert\n        with Image.open(path) as img:\n            img = img.convert(\"RGB\")\n            tensor = pil_to_tensor_safe(img, image_size=self.image_size)\n        labels = []\n        mask = []\n        for c in self.label_cols:\n            v = rec.get(c, None)\n            if v is None or (isinstance(v, float) and np.isnan(v)):\n                labels.append(0.0); mask.append(0.0)\n            else:\n                labels.append(float(v)); mask.append(1.0)\n        labels = torch.tensor(labels, dtype=torch.float32)\n        mask = torch.tensor(mask, dtype=torch.float32)\n        return tensor, labels, mask, path\n\n# Quick sanity check (small batch)\nsample_recs = train_records[:8]\nds_test = CXRRecordsDatasetSafe(sample_recs)\nloader_test = DataLoader(ds_test, batch_size=2, shuffle=False, num_workers=0)\nfor imgs, labs, masks, paths in loader_test:\n    print(\"Sanity batch shapes:\", imgs.shape, labs.shape)\n    break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T05:05:51.402786Z","iopub.execute_input":"2025-11-09T05:05:51.403481Z","iopub.status.idle":"2025-11-09T05:05:51.494644Z","shell.execute_reply.started":"2025-11-09T05:05:51.403459Z","shell.execute_reply":"2025-11-09T05:05:51.493993Z"}},"outputs":[{"name":"stdout","text":"Sanity batch shapes: torch.Size([2, 3, 320, 320]) torch.Size([2, 14])\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ===== GUARANTEED-FAST SMOKE TEST (paste & run) =====\n# 1) If your kernel is still running, interrupt it, then run this cell.\n# 2) This will use a small subset and cap steps per epoch to ensure it's quick.\n\nimport os, random, time\nimport numpy as np\nfrom PIL import Image\nimport torch, torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torchvision.models as models\nimport torch.nn as nn\n\n# ---------- FAST debug CONFIG (edit if needed) ----------\nFAST_NUM_SAMPLES = 800        # small number -> very fast\nBATCH_SIZE = 16               # larger batch => fewer steps\nIMAGE_SIZE = 128              # smaller image => faster per-step\nMAX_STEPS_PER_EPOCH = 100     # cap steps per epoch\nTEST_EPOCHS = 1\nTEST_EXEMPLAR_BUDGET = 10\nTEST_EXEMPLARS_PER_BATCH = 2\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", DEVICE)\n\n# ---------- safe pil->tensor (same as before) ----------\ndef pil_to_tensor_safe(pil_img, image_size=IMAGE_SIZE):\n    if pil_img.mode != \"RGB\":\n        pil_img = pil_img.convert(\"RGB\")\n    pil_img = pil_img.resize((image_size, image_size), resample=Image.BILINEAR)\n    arr = np.asarray(pil_img, dtype=np.uint8)\n    if arr.ndim == 2:\n        arr = np.stack([arr]*3, axis=-1)\n    arr = arr.astype(np.float32) / 255.0\n    mean = np.array([0.485,0.485,0.485], dtype=np.float32)\n    std  = np.array([0.229,0.229,0.229], dtype=np.float32)\n    arr = (arr - mean) / std\n    arr = np.transpose(arr, (2,0,1)).copy()\n    return torch.tensor(arr.tolist(), dtype=torch.float32)\n\n# ---------- load a small subset safely (use resolved CSV if exist) ----------\ndef load_resolved_csv(path=\"/kaggle/working/chexpert_train_resolved_fast.csv\", max_rows=None):\n    import csv\n    recs = []\n    if not os.path.exists(path):\n        raise RuntimeError(f\"Resolved CSV not found at {path}. Run CSV-cleaning cell first.\")\n    with open(path,'r',newline='') as fr:\n        rdr = csv.reader(fr)\n        hdr = next(rdr)\n        for i,r in enumerate(rdr):\n            if max_rows is not None and i >= max_rows: break\n            if len(r) < len(hdr): r = r + [''] * (len(hdr) - len(r))\n            rec = {hdr[j]: (r[j].strip() if r[j] != '' else None) for j in range(len(hdr))}\n            if 'resolved_path' not in rec or not rec['resolved_path']:\n                rec['resolved_path'] = rec.get('Path')\n            recs.append(rec)\n    return recs\n\n# Try to obtain records; prefer preloaded train_records if present but we will slice it down\nif 'train_records' in globals():\n    all_recs = train_records\n    print(\"Using existing train_records (will sample FAST_NUM_SAMPLES from it).\")\nelse:\n    all_recs = load_resolved_csv(max_rows=FAST_NUM_SAMPLES*2)  # read minimal rows\n    print(\"Loaded records from resolved CSV.\")\n\n# Choose a small random subset to be fast and reproducible\nrandom.seed(42)\nif len(all_recs) > FAST_NUM_SAMPLES:\n    sample_recs = random.sample(all_recs, FAST_NUM_SAMPLES)\nelse:\n    sample_recs = all_recs\n\n# ---------- Dataset class (small robust) ----------\nclass SmallCXRDataset(torch.utils.data.Dataset):\n    def __init__(self, records, image_col=\"resolved_path\", image_size=IMAGE_SIZE):\n        self.records = records\n        self.image_col = image_col\n        self.image_size = image_size\n    def __len__(self): return len(self.records)\n    def __getitem__(self, idx):\n        rec = self.records[idx]\n        p = rec.get(self.image_col)\n        if not p:\n            raise RuntimeError(f\"Missing path at idx {idx}\")\n        with Image.open(p) as im:\n            im = im.convert(\"RGB\")\n            t = pil_to_tensor_safe(im, image_size=self.image_size)\n        # Build label vector robustly: convert known numeric columns, handle -1 -> masked\n        labels = []\n        possible = [k for k in rec.keys() if k not in (\"Path\",\"resolved_path\",\"Sex\",\"Age\",\"Frontal/Lateral\",\"AP/PA\")]\n        for k in possible:\n            v = rec.get(k)\n            try:\n                labels.append(float(v) if v not in (None,\"\") else 0.0)\n            except:\n                pass\n        if len(labels) == 0:\n            labels = [0.0]*14\n        labels = np.array(labels, dtype=np.float32)\n        # If CheXpert uses -1 for uncertain, mask them out\n        mask = np.ones_like(labels, dtype=np.float32)\n        mask[labels == -1.0] = 0.0\n        labels[labels == -1.0] = 0.0\n        return t, torch.tensor(labels, dtype=torch.float32), torch.tensor(mask, dtype=torch.float32), p\n\n# ---------- DataLoader ----------\nds = SmallCXRDataset(sample_recs, image_size=IMAGE_SIZE)\nloader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n\n# ---------- Build model with correct num labels ----------\n# infer num labels from first sample\n_, lab0, _, _ = ds[0]\nNUM_LABELS = lab0.numel()\nprint(\"NUM_LABELS inferred:\", NUM_LABELS)\n\ndef small_model(num_labels):\n    m = models.resnet18(pretrained=True)\n    m.fc = nn.Linear(m.fc.in_features, num_labels)\n    return m\n\nmodel = small_model(NUM_LABELS).to(DEVICE)\nopt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n\n# ---------- Small exemplar buffer ----------\nclass ExBuf:\n    def __init__(self, budget):\n        self.paths=[]; self.labels=[]; self.tensors=[]\n        self.budget=budget\n    def add(self, paths, labels, image_size=IMAGE_SIZE):\n        for p,l in zip(paths, labels):\n            self.paths.append(p); self.labels.append(np.array(l,dtype=np.float32))\n            with Image.open(p) as im:\n                tt = pil_to_tensor_safe(im, image_size=image_size)\n            self.tensors.append(tt.cpu())\n        while len(self.paths)>self.budget:\n            self.paths.pop(0); self.labels.pop(0); self.tensors.pop(0)\n    def sample_as_batch(self,k,device='cpu'):\n        if len(self.paths)==0: return None,None\n        k=min(k,len(self.paths)); idx=np.random.choice(len(self.paths),k,replace=False)\n        proc=[]; labs=[]\n        for i in idx:\n            proc.append(self.tensors[i])\n            labs.append(self.labels[i])\n        return torch.stack(proc).to(device), torch.tensor([np.array(x) for x in labs],dtype=torch.float32).to(device)\n\nexbuf = ExBuf(TEST_EXEMPLAR_BUDGET)\n\n# ---------- TRAIN (bounded steps) ----------\nprint(\"Starting FAST smoke test: samples\", len(sample_recs))\nmodel.train()\nglobal_step = 0\nfor epoch in range(TEST_EPOCHS):\n    step = 0\n    for imgs, labels, masks, paths in loader:\n        step += 1; global_step += 1\n        imgs = imgs.to(DEVICE); labels = labels.to(DEVICE); masks = masks.to(DEVICE)\n        ex_t, ex_l = exbuf.sample_as_batch(TEST_EXEMPLARS_PER_BATCH, device=DEVICE)\n        if ex_t is not None:\n            imgs = torch.cat([imgs, ex_t], dim=0)\n            labels = torch.cat([labels, ex_l], dim=0)\n            masks = torch.cat([masks, torch.ones_like(ex_l)], dim=0)\n        opt.zero_grad()\n        out = model(imgs)\n        if out.shape != labels.shape:\n            raise RuntimeError(f\"Output shape {out.shape} != labels {labels.shape}\")\n        bce = F.binary_cross_entropy_with_logits(out, labels, reduction='none')\n        loss = (bce * masks).sum() / (masks.sum().clamp_min(1.0))\n        loss.backward(); opt.step()\n        if step % 5 == 0:\n            print(f\"Epoch {epoch+1} step {step}, loss {loss.item():.4f}\")\n        if MAX_STEPS_PER_EPOCH is not None and step >= MAX_STEPS_PER_EPOCH:\n            print(f\"Reached MAX_STEPS_PER_EPOCH={MAX_STEPS_PER_EPOCH}, breaking epoch early.\")\n            break\n    # add small exemplars from this subset\n    k = min(TEST_EXEMPLAR_BUDGET, len(sample_recs))\n    sample_recs_small = random.sample(sample_recs, k)\n    imgs_to_add = [r.get(\"resolved_path\") for r in sample_recs_small]\n    labs = [[0.0]*NUM_LABELS for _ in imgs_to_add]\n    exbuf.add(imgs_to_add, labs, image_size=IMAGE_SIZE)\n\nprint(\"FAST smoke test finished. global_step:\", global_step)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T05:06:02.684235Z","iopub.execute_input":"2025-11-09T05:06:02.684826Z","iopub.status.idle":"2025-11-09T05:06:15.662599Z","shell.execute_reply.started":"2025-11-09T05:06:02.684802Z","shell.execute_reply":"2025-11-09T05:06:15.661759Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nUsing existing train_records (will sample FAST_NUM_SAMPLES from it).\nNUM_LABELS inferred: 14\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 165MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Starting FAST smoke test: samples 800\nEpoch 1 step 5, loss 0.4488\nEpoch 1 step 10, loss 0.3896\nEpoch 1 step 15, loss 0.3694\nEpoch 1 step 20, loss 0.4342\nEpoch 1 step 25, loss 0.3679\nEpoch 1 step 30, loss 0.4236\nEpoch 1 step 35, loss 0.3796\nEpoch 1 step 40, loss 0.3983\nEpoch 1 step 45, loss 0.2977\nEpoch 1 step 50, loss 0.3144\nFAST smoke test finished. global_step: 50\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"####THIS IS THE FINAL CODE FOR THE WHOLE DATASET BUT I AM JUST CREATING A BASE MODEL FOR FUTURE IMPLEMENTAION","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T18:43:25.549700Z","iopub.status.idle":"2025-11-08T18:43:25.549996Z","shell.execute_reply.started":"2025-11-08T18:43:25.549825Z","shell.execute_reply":"2025-11-08T18:43:25.549836Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell: INCREMENTAL_DATA_PREP\n# CONFIG (edit paths)\nMASTER_RESOLVED_CSV = \"/kaggle/working/chexpert_train_resolved.csv\"  # master resolved CSV used by training\nNEW_FOLDER = None      # e.g. \"/kaggle/input/my_new_chexpert_chunk\" or None\nNEW_RESOLVED_CSV = None # e.g. \"/kaggle/working/new_chunk_resolved.csv\" if you already have a resolved CSV to ingest\nAPPEND_TO_MASTER = True\nCREATE_TASK_SPLITS = True\nN_TASKS = 4\nOUT_TASK_SPLIT_JSON = \"/kaggle/working/task_splits.json\"\n\nimport csv, os, json\n\ndef get_files_under(root, allow_exts=(\".jpg\",\".jpeg\",\".png\")):\n    out = []\n    for d,_,files in os.walk(root):\n        for f in files:\n            if f.lower().endswith(allow_exts):\n                out.append(os.path.join(d,f))\n    return out\n\ndef make_records_from_folder(folder):\n    \"\"\"Return list of record dicts: {'resolved_path':abs, 'Path':relative_like, ...}\"\"\"\n    files = get_files_under(folder)\n    recs = []\n    for p in files:\n        # Try to produce a CSV-style Path similar to CheXpert CSV format:\n        rel = os.path.relpath(p, start=folder).replace(os.path.sep, \"/\")\n        fake_path = os.path.basename(folder).rstrip(\"/\") + \"/\" + rel\n        recs.append({\"Path\": fake_path, \"resolved_path\": p})\n    return recs\n\ndef read_resolved_csv_as_records(path):\n    recs = []\n    with open(path,'r',newline='') as fr:\n        rdr = csv.reader(fr)\n        hdr = next(rdr)\n        for r in rdr:\n            if len(r) < len(hdr):\n                r = r + ['']*(len(hdr)-len(r))\n            rec = {hdr[i]: (r[i].strip() if r[i] != '' else None) for i in range(len(hdr))}\n            if 'resolved_path' not in rec or not rec['resolved_path']:\n                rec['resolved_path'] = rec.get('Path')\n            recs.append(rec)\n    return recs\n\ndef append_records_to_master(new_recs, master_csv=MASTER_RESOLVED_CSV):\n    # append rows to master (preserves header)\n    if not os.path.exists(master_csv):\n        # create master by writing header keys from first rec\n        hdr = list(new_recs[0].keys())\n        with open(master_csv, 'w', newline='') as fw:\n            w = csv.writer(fw)\n            w.writerow(hdr)\n            for r in new_recs:\n                row = [r.get(h, \"\") for h in hdr]\n                w.writerow(row)\n        return\n    # master exists -> read header, append rows matching header order (fill missing)\n    with open(master_csv, 'r', newline='') as fr:\n        rdr = csv.reader(fr)\n        master_hdr = next(rdr)\n    with open(master_csv, 'a', newline='') as fw:\n        w = csv.writer(fw)\n        for r in new_recs:\n            row = [r.get(h, \"\") for h in master_hdr]\n            w.writerow(row)\n\ndef create_task_splits(master_csv=MASTER_RESOLVED_CSV, n_tasks=N_TASKS, out_json=OUT_TASK_SPLIT_JSON):\n    recs = read_resolved_csv_as_records(master_csv)\n    random.shuffle(recs)\n    splits = [ [] for _ in range(n_tasks) ]\n    for i, r in enumerate(recs):\n        splits[i % n_tasks].append(r)\n    # write small manifest (only Path/resolved_path per task)\n    manifest = { f\"task_{i+1}\": [ {\"Path\": rr.get(\"Path\"), \"resolved_path\": rr.get(\"resolved_path\")} for rr in splits[i] ] for i in range(n_tasks) }\n    with open(out_json, 'w') as fw:\n        json.dump(manifest, fw)\n    print(f\"Created {n_tasks} task splits and saved manifest -> {out_json}\")\n    return manifest\n\n# ----- MAIN logic -----\nnew_records = []\nif NEW_FOLDER:\n    if not os.path.exists(NEW_FOLDER):\n        raise FileNotFoundError(f\"NEW_FOLDER not found: {NEW_FOLDER}\")\n    print(\"Scanning NEW_FOLDER for images...\")\n    new_records = make_records_from_folder(NEW_FOLDER)\n    print(\"Found\", len(new_records), \"images in new folder.\")\nelif NEW_RESOLVED_CSV:\n    print(\"Loading new resolved CSV:\", NEW_RESOLVED_CSV)\n    new_records = read_resolved_csv_as_records(NEW_RESOLVED_CSV)\n    print(\"Loaded\", len(new_records), \"records from new resolved CSV.\")\nelse:\n    print(\"No NEW_FOLDER or NEW_RESOLVED_CSV specified. To create incremental dataset, set one of them and re-run.\")\n    new_records = []\n\n# append to master resolved CSV\nif APPEND_TO_MASTER and new_records:\n    print(\"Appending new records to master resolved CSV...\")\n    append_records_to_master(new_records, MASTER_RESOLVED_CSV)\n    print(\"Appended\", len(new_records), \"records to\", MASTER_RESOLVED_CSV)\n\n# create task splits manifest (optional)\nif CREATE_TASK_SPLITS:\n    if not os.path.exists(MASTER_RESOLVED_CSV):\n        raise FileNotFoundError(f\"Master CSV not found: {MASTER_RESOLVED_CSV}\")\n    manifest = create_task_splits(MASTER_RESOLVED_CSV, n_tasks=N_TASKS)\n    print(\"Task splits created. Example sizes:\", {k: len(v) for k,v in manifest.items()})\nelse:\n    print(\"Skipping task split creation.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T05:06:24.578041Z","iopub.execute_input":"2025-11-09T05:06:24.578743Z","iopub.status.idle":"2025-11-09T05:06:27.120786Z","shell.execute_reply.started":"2025-11-09T05:06:24.578718Z","shell.execute_reply":"2025-11-09T05:06:27.119958Z"}},"outputs":[{"name":"stdout","text":"No NEW_FOLDER or NEW_RESOLVED_CSV specified. To create incremental dataset, set one of them and re-run.\nCreated 4 task splits and saved manifest -> /kaggle/working/task_splits.json\nTask splits created. Example sizes: {'task_1': 55854, 'task_2': 55854, 'task_3': 55853, 'task_4': 55853}\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# CELL: DEFINE_EXEMPLAR_AND_EWC_HELPERS\n# Re-defines ExemplarBufferCached, compute_fisher, and ewc_penalty used by the final training cell.\n\nimport os, random, math, time\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import DataLoader\n\n# ---------- Exemplar buffer (robust, caches tensors lazily) ----------\nclass ExemplarBufferCached:\n    def __init__(self, capacity):\n        self.capacity = int(capacity)\n        self.paths = []    # list[str]\n        self.labels = []   # list[list[float]] or list[tensor]\n        self._cache = None # cached tensor batch (rebuild when needed)\n\n    def __len__(self):\n        return len(self.paths)\n\n    def clear_cache(self):\n        self._cache = None\n\n    def add(self, paths, labels, image_size=224, cache_tensors=True):\n        # trim if exceeding capacity (FIFO)\n        n_new = len(paths)\n        if len(self.paths) + n_new > self.capacity:\n            extra = len(self.paths) + n_new - self.capacity\n            # drop oldest\n            self.paths = self.paths[extra:]\n            self.labels = self.labels[extra:]\n        self.paths.extend(paths)\n        self.labels.extend(labels)\n        if cache_tensors:\n            self.clear_cache()\n\n    def sample_as_batch(self, n, device):\n        if len(self.paths) == 0:\n            return None, None\n        n = min(n, len(self.paths))\n        idxs = random.sample(range(len(self.paths)), n)\n        imgs = []\n        labs = []\n        for i in idxs:\n            p = self.paths[i]\n            lab = self.labels[i]\n            # read image, convert to tensor (use PIL)\n            with Image.open(p) as im:\n                im = im.convert(\"RGB\")\n                im = im.resize((224,224))\n                arr = np.asarray(im, dtype=np.float32)/255.0\n                if arr.ndim == 2:\n                    arr = np.stack([arr]*3, -1)\n                arr = (arr - np.array([0.485,0.485,0.485]))/np.array([0.229,0.229,0.229])\n                arr = np.transpose(arr, (2,0,1)).copy()\n                imgs.append(torch.tensor(arr, dtype=torch.float32))\n            # label: accept numpy/list/tensor\n            if isinstance(lab, torch.Tensor):\n                labs.append(lab.clone().float())\n            else:\n                labs.append(torch.tensor(lab, dtype=torch.float32))\n        imgs = torch.stack(imgs).to(device)\n        labs = torch.stack(labs).to(device)\n        return imgs, labs\n\n# ---------- Fisher computation for EWC ----------\ndef compute_fisher(model, dataloader, device, samples=200):\n    \"\"\"\n    Approximate diagonal Fisher information for parameters in model (only for params with requires_grad=True).\n    Returns: dict{name -> fisher_tensor} with same device as parameters.\n    \"\"\"\n    model.eval()\n    # initialize fisher accumulators\n    fisher = {}\n    count = 0\n    criterion = torch.nn.BCEWithLogitsLoss(reduction='none')\n\n    # iterate dataloader until we've processed ~samples examples\n    with torch.no_grad():\n        for imgs, labels, masks, _ in dataloader:\n            imgs = imgs.to(device); labels = labels.to(device); masks = masks.to(device)\n            # skip batches with no valid labels\n            if masks.sum().item() == 0:\n                continue\n            # forward (we need gradients for fisher, so temporarily enable grads)\n            imgs.requires_grad = False\n            # compute per-sample loss (sum across labels & samples) so grad of params is meaningful\n            out = model(imgs)\n            # compute elementwise BCE, mask it, then sum to scalar\n            bce = criterion(out, labels)\n            loss = (bce * masks).sum()\n            # compute gradients w.r.t. parameters\n            # enable grad for parameters temporarily\n            model.zero_grad()\n            # use autograd to get param grads\n            grads = torch.autograd.grad(loss, [p for p in model.parameters() if p.requires_grad], retain_graph=False, allow_unused=True)\n            # accumulate squared gradients\n            i=0\n            for p in [p for p in model.parameters() if p.requires_grad]:\n                g = grads[i]\n                if g is None:\n                    i += 1; continue\n                g2 = (g.detach() ** 2)\n                name = None\n                # try to find parameter name mapping by id (best-effort)\n                # We'll key by id(p) to avoid needing string names\n                key = id(p)\n                if key not in fisher:\n                    fisher[key] = torch.zeros_like(p.detach(), device=p.device)\n                fisher[key] += g2.to(p.device)\n                i += 1\n            count += imgs.shape[0]\n            if count >= samples:\n                break\n\n    # average\n    if count == 0:\n        # fallback: small uniform fisher to avoid div-by-zero later\n        for p in model.parameters():\n            if p.requires_grad:\n                fisher[id(p)] = torch.ones_like(p.detach(), device=p.device) * 1e-6\n        return fisher\n\n    for k in fisher:\n        fisher[k] /= float(count)\n    return fisher\n\n# ---------- EWC penalty calculation ----------\ndef ewc_penalty(model, fisher, old_params, ewc_lambda):\n    \"\"\"\n    fisher: dict keyed by id(param) -> fisher_tensor (same shape as param)\n    old_params: dict keyed by id(param) -> tensor (detached copy of param at previous task)\n    returns scalar penalty\n    \"\"\"\n    loss = 0.0\n    for p in model.parameters():\n        if not p.requires_grad:\n            continue\n        key = id(p)\n        if key not in fisher or key not in old_params:\n            continue\n        f = fisher[key].to(p.device)\n        old = old_params[key].to(p.device)\n        loss += (f * (p - old).pow(2)).sum()\n    # standard EWC uses 0.5 * lambda * sum f * (theta - theta_old)^2\n    return 0.5 * ewc_lambda * loss\n\n# ---------- small utility to create old_params snapshot keyed by id ----------\ndef snapshot_params(model):\n    return { id(p): p.clone().detach().to(p.device) for p in model.parameters() if p.requires_grad }\n\n# expose names into global namespace expected by training cell\nglobals()['ExemplarBufferCached'] = ExemplarBufferCached\nglobals()['compute_fisher'] = compute_fisher\nglobals()['ewc_penalty'] = ewc_penalty\nglobals()['snapshot_params'] = snapshot_params\n\nprint(\"Helpers defined: ExemplarBufferCached, compute_fisher, ewc_penalty, snapshot_params\")\nprint(\"Now re-run your final training cell (the large training cell).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T05:14:31.467430Z","iopub.execute_input":"2025-11-09T05:14:31.468035Z","iopub.status.idle":"2025-11-09T05:14:31.486012Z","shell.execute_reply.started":"2025-11-09T05:14:31.467999Z","shell.execute_reply":"2025-11-09T05:14:31.485458Z"}},"outputs":[{"name":"stdout","text":"Helpers defined: ExemplarBufferCached, compute_fisher, ewc_penalty, snapshot_params\nNow re-run your final training cell (the large training cell).\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# Cell A: Inspect filtered CSV header & first row\nFILTERED_MASTER = \"/kaggle/working/chexpert_train_resolved_filtered_for_train.csv\"  # path from your run\nimport csv, itertools, os, json\nassert os.path.exists(FILTERED_MASTER), f\"Missing {FILTERED_MASTER}\"\nwith open(FILTERED_MASTER,'r',newline='') as fr:\n    rdr = csv.reader(fr)\n    header = next(rdr)\n    first = next(rdr)\nprint(\"Header columns (count):\", len(header))\nfor i,h in enumerate(header[:80]):   # print first 80 cols if many\n    print(i, repr(h))\nprint(\"\\nFirst row (first 80 cols):\")\nfor i,v in enumerate(first[:80]):\n    print(i, repr(v))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T05:33:42.167708Z","iopub.execute_input":"2025-11-09T05:33:42.168424Z","iopub.status.idle":"2025-11-09T05:33:42.174697Z","shell.execute_reply.started":"2025-11-09T05:33:42.168395Z","shell.execute_reply":"2025-11-09T05:33:42.174007Z"}},"outputs":[{"name":"stdout","text":"Header columns (count): 20\n0 'Path'\n1 'Sex'\n2 'Age'\n3 'Frontal/Lateral'\n4 'AP/PA'\n5 'No Finding'\n6 'Enlarged Cardiomediastinum'\n7 'Cardiomegaly'\n8 'Lung Opacity'\n9 'Lung Lesion'\n10 'Edema'\n11 'Consolidation'\n12 'Pneumonia'\n13 'Atelectasis'\n14 'Pneumothorax'\n15 'Pleural Effusion'\n16 'Pleural Other'\n17 'Fracture'\n18 'Support Devices'\n19 'resolved_path'\n\nFirst row (first 80 cols):\n0 'CheXpert-v1.0-small/train/patient00001/study1/view1_frontal.jpg'\n1 'Female'\n2 '68'\n3 'Frontal'\n4 'AP'\n5 '1.0'\n6 ''\n7 ''\n8 ''\n9 ''\n10 ''\n11 ''\n12 ''\n13 ''\n14 '0.0'\n15 ''\n16 ''\n17 ''\n18 '1.0'\n19 '/kaggle/input/chexpert/train/patient00001/study1/view1_frontal.jpg'\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# Cell B: Auto-detect + map label column names, rewrite CSV, and show counts\nfrom collections import defaultdict\nimport csv, os, difflib\n\nFILTERED_MASTER = \"/kaggle/working/chexpert_train_resolved_filtered_for_train.csv\"\nMAPPED_CSV = \"/kaggle/working/chexpert_train_resolved_mapped.csv\"\ncanonical = [\n    \"No Finding\",\"Enlarged Cardiomediastinum\",\"Cardiomegaly\",\"Lung Opacity\",\n    \"Lung Lesion\",\"Edema\",\"Consolidation\",\"Pneumonia\",\"Atelectasis\",\n    \"Pneumothorax\",\"Pleural Effusion\",\"Pleural Other\",\"Fracture\",\"Support Devices\"\n]\n\ndef normalize(name):\n    if name is None: return \"\"\n    return \"\".join(name.lower().strip().split())  # lower, strip spaces, remove internal spaces\n\nwith open(FILTERED_MASTER,'r',newline='') as fr:\n    rdr = csv.DictReader(fr)\n    orig_fields = rdr.fieldnames\n    norm_map = {f: normalize(f) for f in orig_fields}\n\n# build lookup: for each canonical, find best matching original header by normalized string or difflib\nfield_by_canon = {}\nfor c in canonical:\n    norm = normalize(c)\n    # exact normalized match\n    candidates = [f for f,fn in norm_map.items() if fn == norm]\n    if candidates:\n        field_by_canon[c] = candidates[0]\n        continue\n    # try contains / partial match\n    candidates = [f for f,fn in norm_map.items() if norm in fn or fn in norm]\n    if candidates:\n        field_by_canon[c] = candidates[0]\n        continue\n    # fuzzy by sequence matcher\n    best = difflib.get_close_matches(norm, list(norm_map.values()), n=1, cutoff=0.6)\n    if best:\n        # map back to original field name\n        chosen_norm = best[0]\n        orig = [f for f,fn in norm_map.items() if fn == chosen_norm][0]\n        field_by_canon[c] = orig\n        continue\n    # not found\n    field_by_canon[c] = None\n\nprint(\"Mapping canonical label -> CSV header found:\")\nfor c in canonical:\n    print(c, \"->\", field_by_canon[c])\n\n# Now rewrite CSV ensuring canonical columns exist\nwith open(FILTERED_MASTER,'r',newline='') as fr, open(MAPPED_CSV,'w',newline='') as fw:\n    rdr = csv.DictReader(fr)\n    # new header: original fields + ensure canonical label columns (if mapped to different names we'll keep both)\n    out_fields = list(rdr.fieldnames)\n    # if mapping used a different header name, we'll add canonical name as alias column\n    for c in canonical:\n        mapped = field_by_canon[c]\n        if mapped is None:\n            # add empty canonical column\n            if c not in out_fields:\n                out_fields.append(c)\n        else:\n            if mapped != c and c not in out_fields:\n                out_fields.append(c)\n    writer = csv.DictWriter(fw, fieldnames=out_fields)\n    writer.writeheader()\n    for r in rdr:\n        out = dict(r)\n        for c in canonical:\n            mapped = field_by_canon[c]\n            if mapped is None:\n                out[c] = \"\"\n            else:\n                out[c] = r.get(mapped, \"\")\n        writer.writerow(out)\n\nprint(\"Wrote mapped CSV:\", MAPPED_CSV)\n\n# Quick counts for canonical labels (scan first 50k lines for speed)\ncounts = {c: {\"pos\":0,\"zero\":0,\"uncertain\":0,\"total\":0} for c in canonical}\nNSCAN = 50000\nwith open(MAPPED_CSV,'r',newline='') as fr:\n    rdr = csv.DictReader(fr)\n    for i,row in enumerate(rdr):\n        if i >= NSCAN: break\n        for c in canonical:\n            v = row.get(c, \"\")\n            if v is None or v == \"\": continue\n            try:\n                f = float(v)\n            except:\n                continue\n            counts[c][\"total\"] += 1\n            if f == 1.0: counts[c][\"pos\"] += 1\n            elif f == 0.0: counts[c][\"zero\"] += 1\n            elif f == -1.0: counts[c][\"uncertain\"] += 1\n\nprint(\"Label counts (scanned up to\", NSCAN, \"rows):\")\nfor c in canonical:\n    print(c, counts[c])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T05:34:04.916648Z","iopub.execute_input":"2025-11-09T05:34:04.916911Z","iopub.status.idle":"2025-11-09T05:34:08.769196Z","shell.execute_reply.started":"2025-11-09T05:34:04.916891Z","shell.execute_reply":"2025-11-09T05:34:08.768599Z"}},"outputs":[{"name":"stdout","text":"Mapping canonical label -> CSV header found:\nNo Finding -> No Finding\nEnlarged Cardiomediastinum -> Enlarged Cardiomediastinum\nCardiomegaly -> Cardiomegaly\nLung Opacity -> Lung Opacity\nLung Lesion -> Lung Lesion\nEdema -> Edema\nConsolidation -> Consolidation\nPneumonia -> Pneumonia\nAtelectasis -> Atelectasis\nPneumothorax -> Pneumothorax\nPleural Effusion -> Pleural Effusion\nPleural Other -> Pleural Other\nFracture -> Fracture\nSupport Devices -> Support Devices\nWrote mapped CSV: /kaggle/working/chexpert_train_resolved_mapped.csv\nLabel counts (scanned up to 50000 rows):\nNo Finding {'pos': 5869, 'zero': 0, 'uncertain': 0, 'total': 5869}\nEnlarged Cardiomediastinum {'pos': 2398, 'zero': 6071, 'uncertain': 2715, 'total': 11184}\nCardiomegaly {'pos': 6333, 'zero': 3097, 'uncertain': 1825, 'total': 11255}\nLung Opacity {'pos': 22374, 'zero': 1767, 'uncertain': 1245, 'total': 25386}\nLung Lesion {'pos': 2262, 'zero': 407, 'uncertain': 384, 'total': 3053}\nEdema {'pos': 10236, 'zero': 5166, 'uncertain': 2576, 'total': 17978}\nConsolidation {'pos': 3292, 'zero': 7804, 'uncertain': 5703, 'total': 16799}\nPneumonia {'pos': 1571, 'zero': 775, 'uncertain': 4017, 'total': 6363}\nAtelectasis {'pos': 6924, 'zero': 311, 'uncertain': 7001, 'total': 14236}\nPneumothorax {'pos': 4538, 'zero': 12896, 'uncertain': 646, 'total': 18080}\nPleural Effusion {'pos': 18157, 'zero': 9747, 'uncertain': 2512, 'total': 30416}\nPleural Other {'pos': 1040, 'zero': 85, 'uncertain': 614, 'total': 1739}\nFracture {'pos': 2193, 'zero': 701, 'uncertain': 151, 'total': 3045}\nSupport Devices {'pos': 23792, 'zero': 1321, 'uncertain': 205, 'total': 25318}\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# Cell C: Replace master CSV path used by your training cell and rebuild task_recs_list (quick)\nMAPPED_CSV = \"/kaggle/working/chexpert_train_resolved_mapped.csv\"\nassert os.path.exists(MAPPED_CSV)\n\n# quick read all records (lightweight dicts)\nall_recs = []\nwith open(MAPPED_CSV,'r',newline='') as fr:\n    rdr = csv.DictReader(fr)\n    for r in rdr:\n        if 'resolved_path' not in r or not r['resolved_path']:\n            r['resolved_path'] = r.get('Path','')\n        all_recs.append(r)\n\nprint(\"Total records in mapped CSV:\", len(all_recs))\n\n# Re-create task splits (4 tasks) and small validation sample for health checks\nimport math, random\nrandom.shuffle(all_recs)\nN_TASKS = 4\nper = math.ceil(len(all_recs)/N_TASKS)\ntask_recs_list = [ all_recs[i*per:(i+1)*per] for i in range(N_TASKS) ]\nval_size = min(2000, max(200, int(0.05 * len(all_recs))))\nval_sample = random.sample(all_recs, val_size)\nprint(\"Rebuilt tasks. samples per task:\", [len(t) for t in task_recs_list[:4]])\nprint(\"New val_sample size:\", len(val_sample))\n\n# Quick check: label presence counts across val_sample\ndef label_presence(records, label_cols):\n    counts={c:{\"pos\":0,\"zero\":0,\"uncertain\":0,\"total\":0} for c in label_cols}\n    for r in records:\n        for c in label_cols:\n            v = r.get(c,\"\")\n            if v is None or v == \"\": continue\n            try:\n                f = float(v)\n            except:\n                continue\n            counts[c][\"total\"] += 1\n            if f == 1.0: counts[c][\"pos\"] += 1\n            elif f == 0.0: counts[c][\"zero\"] += 1\n            elif f == -1.0: counts[c][\"uncertain\"] += 1\n    return counts\n\ncounts = label_presence(val_sample, canonical)\nfor c in canonical:\n    print(c, counts[c])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T05:34:51.540432Z","iopub.execute_input":"2025-11-09T05:34:51.540957Z","iopub.status.idle":"2025-11-09T05:34:52.819121Z","shell.execute_reply.started":"2025-11-09T05:34:51.540934Z","shell.execute_reply":"2025-11-09T05:34:52.818511Z"}},"outputs":[{"name":"stdout","text":"Total records in mapped CSV: 220854\nRebuilt tasks. samples per task: [55214, 55214, 55214, 55212]\nNew val_sample size: 2000\nNo Finding {'pos': 208, 'zero': 0, 'uncertain': 0, 'total': 208}\nEnlarged Cardiomediastinum {'pos': 100, 'zero': 197, 'uncertain': 122, 'total': 419}\nCardiomegaly {'pos': 235, 'zero': 114, 'uncertain': 64, 'total': 413}\nLung Opacity {'pos': 941, 'zero': 63, 'uncertain': 40, 'total': 1044}\nLung Lesion {'pos': 95, 'zero': 8, 'uncertain': 13, 'total': 116}\nEdema {'pos': 444, 'zero': 216, 'uncertain': 102, 'total': 762}\nConsolidation {'pos': 140, 'zero': 245, 'uncertain': 260, 'total': 645}\nPneumonia {'pos': 55, 'zero': 27, 'uncertain': 137, 'total': 219}\nAtelectasis {'pos': 315, 'zero': 11, 'uncertain': 301, 'total': 627}\nPneumothorax {'pos': 186, 'zero': 541, 'uncertain': 26, 'total': 753}\nPleural Effusion {'pos': 802, 'zero': 338, 'uncertain': 89, 'total': 1229}\nPleural Other {'pos': 16, 'zero': 3, 'uncertain': 21, 'total': 40}\nFracture {'pos': 80, 'zero': 24, 'uncertain': 9, 'total': 113}\nSupport Devices {'pos': 1085, 'zero': 46, 'uncertain': 5, 'total': 1136}\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# SWITCH_TO_MAPPED_CSV (run immediately after interrupting the long training cell)\nimport os, csv, random, math\nfrom torch.utils.data import DataLoader\n\nMAPPED_CSV = \"/kaggle/working/chexpert_train_resolved_mapped.csv\"\nassert os.path.exists(MAPPED_CSV), \"Mapped CSV not found: \" + MAPPED_CSV\nprint(\"Using mapped CSV:\", MAPPED_CSV)\n\n# Read all records (lightweight dicts)\nall_recs = []\nwith open(MAPPED_CSV, 'r', newline='') as fr:\n    rdr = csv.DictReader(fr)\n    for r in rdr:\n        if 'resolved_path' not in r or not r['resolved_path']:\n            r['resolved_path'] = r.get('Path','')\n        all_recs.append(r)\nprint(\"Total records in mapped CSV:\", len(all_recs))\n\n# Re-create task_recs_list (4 tasks) and validation sample\nrandom.shuffle(all_recs)\nN_TASKS = 4\nper = math.ceil(len(all_recs)/N_TASKS)\ntask_recs_list = [ all_recs[i*per:(i+1)*per] for i in range(N_TASKS) ]\n\nval_size = min(2000, max(200, int(0.05 * len(all_recs))))\nval_sample = random.sample(all_recs, val_size)\n\n# build val_loader using the dataset class already in kernel (CXRRecordsDataset)\ntry:\n    val_loader = DataLoader(CXRRecordsDataset(val_sample, image_col=\"resolved_path\", image_size=224),\n                            batch_size=16, shuffle=False, num_workers=0, pin_memory=False)\n    print(\"val_loader created (bsize=16, num_workers=0)\")\nexcept Exception as e:\n    print(\"Warning: couldn't create val_loader — error:\", e)\n    val_loader = None\n\n# Print label presence for sanity check (counts)\nlabel_cols = [\n    \"No Finding\",\"Enlarged Cardiomediastinum\",\"Cardiomegaly\",\"Lung Opacity\",\n    \"Lung Lesion\",\"Edema\",\"Consolidation\",\"Pneumonia\",\"Atelectasis\",\n    \"Pneumothorax\",\"Pleural Effusion\",\"Pleural Other\",\"Fracture\",\"Support Devices\"\n]\ndef label_presence(records):\n    counts={c:{\"pos\":0,\"zero\":0,\"uncertain\":0,\"total\":0} for c in label_cols}\n    for r in records:\n        for c in label_cols:\n            v = r.get(c,\"\")\n            if v is None or v == \"\": continue\n            try:\n                f = float(v)\n            except:\n                continue\n            counts[c][\"total\"] += 1\n            if f == 1.0: counts[c][\"pos\"] += 1\n            elif f == 0.0: counts[c][\"zero\"] += 1\n            elif f == -1.0: counts[c][\"uncertain\"] += 1\n    return counts\n\ncounts = label_presence(val_sample)\nprint(\"Validation label presence (pos/zero/uncertain/total):\")\nfor c in label_cols:\n    print(c, counts[c])\n\n# Expose these variables into the notebook globals so your training cell sees them\nglobals()['task_recs_list'] = task_recs_list\nglobals()['val_sample'] = val_sample\nglobals()['val_loader'] = val_loader\nglobals()['MASTER_RESOLVED_CSV'] = MAPPED_CSV\nprint(\"Done. Now re-run the final training cell (it will use the mapped CSV / val_loader).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T05:39:55.165953Z","iopub.execute_input":"2025-11-09T05:39:55.166238Z","iopub.status.idle":"2025-11-09T05:39:56.560706Z","shell.execute_reply.started":"2025-11-09T05:39:55.166217Z","shell.execute_reply":"2025-11-09T05:39:56.560047Z"}},"outputs":[{"name":"stdout","text":"Using mapped CSV: /kaggle/working/chexpert_train_resolved_mapped.csv\nTotal records in mapped CSV: 220854\nval_loader created (bsize=16, num_workers=0)\nValidation label presence (pos/zero/uncertain/total):\nNo Finding {'pos': 190, 'zero': 0, 'uncertain': 0, 'total': 190}\nEnlarged Cardiomediastinum {'pos': 108, 'zero': 188, 'uncertain': 99, 'total': 395}\nCardiomegaly {'pos': 259, 'zero': 83, 'uncertain': 85, 'total': 427}\nLung Opacity {'pos': 962, 'zero': 53, 'uncertain': 45, 'total': 1060}\nLung Lesion {'pos': 88, 'zero': 10, 'uncertain': 11, 'total': 109}\nEdema {'pos': 516, 'zero': 188, 'uncertain': 121, 'total': 825}\nConsolidation {'pos': 133, 'zero': 260, 'uncertain': 257, 'total': 650}\nPneumonia {'pos': 51, 'zero': 21, 'uncertain': 150, 'total': 222}\nAtelectasis {'pos': 275, 'zero': 11, 'uncertain': 294, 'total': 580}\nPneumothorax {'pos': 170, 'zero': 487, 'uncertain': 34, 'total': 691}\nPleural Effusion {'pos': 810, 'zero': 320, 'uncertain': 89, 'total': 1219}\nPleural Other {'pos': 32, 'zero': 1, 'uncertain': 22, 'total': 55}\nFracture {'pos': 67, 'zero': 16, 'uncertain': 1, 'total': 84}\nSupport Devices {'pos': 1061, 'zero': 62, 'uncertain': 7, 'total': 1130}\nDone. Now re-run the final training cell (it will use the mapped CSV / val_loader).\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# ONE-SHOT FINAL TRAINING CELL (uses mapped CSV)\n# Path settings (change only if your files are elsewhere)\nMAPPED_CSV = \"/kaggle/working/chexpert_train_resolved_mapped.csv\"\nTASK_MANIFEST_JSON = \"/kaggle/working/task_splits.json\"   # optional, ignored if missing\nSAVE_DIR = \"/kaggle/working/outputs\"\nos.makedirs(SAVE_DIR, exist_ok=True)\n\n# -------- Imports & basic utils --------\nimport os, csv, time, math, json, random\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\nfrom torchvision import models\nfrom collections import defaultdict\nfrom torch.cuda.amp import autocast, GradScaler\n\n# -------- CONFIG (safe defaults) --------\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nBATCH_SIZE = 16\nIMAGE_SIZE = 224\nNUM_WORKERS = 0     # IMPORTANT: 0 avoids DataLoader worker crashes in Kaggle\nPIN_MEMORY = False\nEPOCHS = 2         # set higher for full accuracy\nEXEMPLAR_BUDGET = 1200\nEXEMPLARS_PER_BATCH = 24\nUSE_EWC = True\nFISHER_SAMPLES = 300\nEWC_LAMBDA = 1000.0\nRANDOM_SEED = 42\n\ntorch.manual_seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\n\nprint(\"Device:\", DEVICE, \"| batch:\", BATCH_SIZE, \"img:\", IMAGE_SIZE, \"num_workers:\", NUM_WORKERS)\n\n# -------- Canonical labels (CheXpert 14) --------\nlabel_cols = [\n    \"No Finding\",\"Enlarged Cardiomediastinum\",\"Cardiomegaly\",\"Lung Opacity\",\n    \"Lung Lesion\",\"Edema\",\"Consolidation\",\"Pneumonia\",\"Atelectasis\",\n    \"Pneumothorax\",\"Pleural Effusion\",\"Pleural Other\",\"Fracture\",\"Support Devices\"\n]\nNUM_LABELS = len(label_cols)\nprint(\"NUM_LABELS =\", NUM_LABELS)\n\n# -------- Image conversion (robust) --------\ndef pil_to_tensor_safe(pil_img, image_size=IMAGE_SIZE):\n    pil_img = pil_img.resize((image_size, image_size))\n    arr = np.asarray(pil_img)\n    if arr.ndim == 2:\n        arr = np.stack([arr]*3, axis=-1)\n    arr = arr.astype(np.float32) / 255.0\n    mean = np.array([0.485,0.485,0.485], dtype=np.float32)\n    std  = np.array([0.229,0.229,0.229], dtype=np.float32)\n    arr = (arr - mean)/std\n    arr = np.transpose(arr, (2,0,1)).copy()\n    return torch.tensor(arr, dtype=torch.float32)\n\n# -------- Dataset (fixed labels) --------\nclass CXRRecordsDatasetSafe(Dataset):\n    def __init__(self, records, image_col=\"resolved_path\", image_size=IMAGE_SIZE):\n        self.records = records\n        self.image_col = image_col\n        self.image_size = image_size\n    def __len__(self):\n        return len(self.records)\n    def __getitem__(self, idx):\n        rec = self.records[idx]\n        p = rec.get(self.image_col) or rec.get(\"Path\")\n        if not p or not os.path.exists(p):\n            raise FileNotFoundError(f\"Missing image for idx={idx}: {p}\")\n        with Image.open(p) as im:\n            im = im.convert(\"RGB\")\n            img_t = pil_to_tensor_safe(im, image_size=self.image_size)\n        labs = []\n        for c in label_cols:\n            v = rec.get(c, \"\")\n            try:\n                labs.append(float(v) if v not in (None,\"\") else 0.0)\n            except:\n                labs.append(0.0)\n        if len(labs) < NUM_LABELS:\n            labs += [0.0] * (NUM_LABELS - len(labs))\n        elif len(labs) > NUM_LABELS:\n            labs = labs[:NUM_LABELS]\n        labels = np.array(labs, dtype=np.float32)\n        mask = np.ones_like(labels, dtype=np.float32)\n        mask[labels == -1.0] = 0.0\n        labels[labels == -1.0] = 0.0\n        return img_t, torch.tensor(labels, dtype=torch.float32), torch.tensor(mask, dtype=torch.float32), p\n\n# Expose name used previously\nCXRRecordsDataset = CXRRecordsDatasetSafe\n\n# -------- Model factory --------\ndef get_densenet(num_labels=NUM_LABELS, pretrained=True):\n    # uses torchvision weights enum if available\n    try:\n        model = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT if pretrained else None)\n    except Exception:\n        model = models.densenet121(pretrained=pretrained)\n    in_f = model.classifier.in_features\n    model.classifier = nn.Linear(in_f, num_labels)\n    return model\n\n# -------- Exemplar buffer + EWC helpers --------\nclass ExemplarBufferCached:\n    def __init__(self, capacity):\n        self.capacity = int(capacity)\n        self.paths = []\n        self.labels = []\n    def __len__(self):\n        return len(self.paths)\n    def add(self, paths, labels):\n        n_new = len(paths)\n        if len(self.paths) + n_new > self.capacity:\n            extra = len(self.paths) + n_new - self.capacity\n            self.paths = self.paths[extra:]\n            self.labels = self.labels[extra:]\n        self.paths.extend(paths)\n        self.labels.extend(labels)\n    def sample_as_batch(self, n, device):\n        if len(self.paths) == 0: return None, None\n        n = min(n, len(self.paths))\n        idxs = random.sample(range(len(self.paths)), n)\n        imgs=[]; labs=[]\n        for i in idxs:\n            p = self.paths[i]\n            with Image.open(p) as im:\n                im = im.convert(\"RGB\")\n                im = im.resize((IMAGE_SIZE, IMAGE_SIZE))\n                arr = np.asarray(im, dtype=np.float32)/255.0\n                if arr.ndim == 2: arr = np.stack([arr]*3, -1)\n                arr = (arr - np.array([0.485,0.485,0.485]))/np.array([0.229,0.229,0.229])\n                arr = np.transpose(arr, (2,0,1)).copy()\n                imgs.append(torch.tensor(arr, dtype=torch.float32))\n            lab = self.labels[i]\n            if isinstance(lab, torch.Tensor): labs.append(lab.float())\n            else: labs.append(torch.tensor(lab, dtype=torch.float32))\n        imgs = torch.stack(imgs).to(device)\n        labs = torch.stack(labs).to(device)\n        return imgs, labs\n\ndef snapshot_params(model):\n    return { id(p): p.clone().detach().to(p.device) for p in model.parameters() if p.requires_grad }\n\ndef compute_fisher(model, dataloader, device, samples=200):\n    model.eval()\n    fisher = {}\n    count = 0\n    criterion = torch.nn.BCEWithLogitsLoss(reduction='none')\n    for imgs, labels, masks, _ in dataloader:\n        imgs = imgs.to(device); labels = labels.to(device); masks = masks.to(device)\n        if masks.sum().item() == 0: continue\n        out = model(imgs)\n        bce = criterion(out, labels)\n        loss = (bce * masks).sum()\n        model.zero_grad()\n        grads = torch.autograd.grad(loss, [p for p in model.parameters() if p.requires_grad], retain_graph=False, allow_unused=True)\n        j=0\n        for p in [p for p in model.parameters() if p.requires_grad]:\n            g = grads[j]\n            j += 1\n            if g is None: continue\n            key = id(p)\n            if key not in fisher: fisher[key] = torch.zeros_like(p.detach(), device=p.device)\n            fisher[key] += (g.detach() ** 2).to(p.device)\n        count += imgs.shape[0]\n        if count >= samples: break\n    if count == 0:\n        for p in model.parameters():\n            if p.requires_grad:\n                fisher[id(p)] = torch.ones_like(p.detach(), device=p.device) * 1e-6\n        return fisher\n    for k in fisher: fisher[k] /= float(count)\n    return fisher\n\ndef ewc_penalty(model, fisher, old_params, ewc_lambda):\n    loss = 0.0\n    for p in model.parameters():\n        if not p.requires_grad: continue\n        key = id(p)\n        if key not in fisher or key not in old_params: continue\n        f = fisher[key].to(p.device); old = old_params[key].to(p.device)\n        loss += (f * (p - old).pow(2)).sum()\n    return 0.5 * ewc_lambda * loss\n\n# -------- Safe AUROC evaluation (non-numpy-sklearn) --------\ndef _auc_from_ranks(y_true, y_score):\n    y_true = np.asarray(y_true); y_score = np.asarray(y_score)\n    n_pos = int(np.sum(y_true==1)); n_neg = int(np.sum(y_true==0))\n    if n_pos == 0 or n_neg == 0: return float('nan')\n    order = np.argsort(y_score); ranks = np.empty_like(order, dtype=float)\n    sorted_scores = y_score[order]\n    i = 0; rank = 1; N = len(y_score)\n    while i < N:\n        j = i+1\n        while j < N and sorted_scores[j] == sorted_scores[i]: j += 1\n        avg_rank = (rank + (rank + (j-i) - 1)) / 2.0\n        ranks[i:j] = avg_rank\n        rank += (j - i); i = j\n    inv_ranks = np.empty_like(ranks); inv_ranks[order] = ranks\n    sum_ranks_pos = np.sum(inv_ranks[y_true==1])\n    auc = (sum_ranks_pos - n_pos * (n_pos + 1) / 2.0) / (n_pos * n_neg)\n    return float(auc)\n\ndef safe_evaluate_auroc(model, dataloader, device, num_labels):\n    model.eval()\n    preds=[]; targets=[]\n    with torch.no_grad():\n        for imgs, labels, masks, _ in dataloader:\n            imgs = imgs.to(device)\n            out = torch.sigmoid(model(imgs)).cpu().numpy()\n            preds.append(out)\n            targets.append(labels.numpy())\n    if len(preds) == 0:\n        return {i: float('nan') for i in range(num_labels)}, float('nan')\n    preds = np.vstack(preds); targets = np.vstack(targets)\n    aucs = {}\n    for i in range(num_labels):\n        y_true = targets[:, i]\n        n_pos = int((y_true >= 0.5).sum()); n_neg = int((y_true < 0.5).sum())\n        if n_pos == 0 or n_neg == 0:\n            aucs[i] = float('nan'); continue\n        y_bin = (y_true >= 0.5).astype(int)\n        s = preds[:, i]\n        aucs[i] = _auc_from_ranks(y_bin, s)\n    vals = [v for v in aucs.values() if not np.isnan(v)]\n    mean_auc = float(np.mean(vals)) if len(vals) > 0 else float('nan')\n    return aucs, mean_auc\n\n# -------- Read mapped CSV, build tasks & validation --------\nassert os.path.exists(MAPPED_CSV), f\"Mapped CSV not found: {MAPPED_CSV}\"\nrecords = []\nwith open(MAPPED_CSV,'r',newline='') as fr:\n    rdr = csv.DictReader(fr)\n    for r in rdr:\n        if 'resolved_path' not in r or not r['resolved_path']:\n            r['resolved_path'] = r.get('Path','')\n        records.append(r)\nprint(\"Records loaded from mapped CSV:\", len(records))\nrandom.shuffle(records)\n\n# If task manifest exists, try to use it; otherwise split into 4 tasks\nif os.path.exists(TASK_MANIFEST_JSON):\n    with open(TASK_MANIFEST_JSON,'r') as fr:\n        tman = json.load(fr)\n    task_recs_list = [tman[k] for k in sorted(tman.keys())]\n    print(\"Loaded task manifest with\", len(task_recs_list), \"tasks.\")\nelse:\n    N_TASKS = 4\n    per = math.ceil(len(records)/N_TASKS)\n    task_recs_list = [ records[i*per:(i+1)*per] for i in range(N_TASKS) ]\n    print(\"Split mapped CSV into\", len(task_recs_list), \"tasks.\")\n\nfor i,t in enumerate(task_recs_list[:4]):\n    print(f\" Task {i+1} samples: {len(t)}\")\n\n# Build validation sample and loader\nval_size = min(2000, max(200, int(0.05 * len(records))))\nval_sample = random.sample(records, val_size)\nval_loader = DataLoader(CXRRecordsDataset(val_sample, image_col=\"resolved_path\", image_size=IMAGE_SIZE),\n                        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\nprint(\"Validation sample size:\", val_size)\n\n# print label presence summary\ndef label_presence_stats(records):\n    counts = {c: {\"pos\":0,\"zero\":0,\"uncertain\":0,\"total\":0} for c in label_cols}\n    for r in records:\n        for c in label_cols:\n            v = r.get(c,\"\")\n            if v is None or v == \"\": continue\n            try:\n                f = float(v)\n            except:\n                continue\n            counts[c][\"total\"] += 1\n            if f == 1.0: counts[c][\"pos\"] += 1\n            elif f == 0.0: counts[c][\"zero\"] += 1\n            elif f == -1.0: counts[c][\"uncertain\"] += 1\n    return counts\n\nvp = label_presence_stats(val_sample)\nprint(\"Validation label presence (pos/zero/uncertain/total):\")\nfor c in label_cols:\n    print(c[:20].ljust(20), vp[c])\n\n# -------- Build model, optimizer, exemplar buffer, scaler --------\nmodel = get_densenet(num_labels=NUM_LABELS, pretrained=True).to(DEVICE)\n# freeze features initially, unfreeze later\nif hasattr(model, \"features\"):\n    for p in model.features.parameters(): p.requires_grad = False\nfor p in model.classifier.parameters(): p.requires_grad = True\n\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4, weight_decay=1e-2)\nscaler = GradScaler()\nexemplar_buffer = ExemplarBufferCached(EXEMPLAR_BUDGET)\nfisher = None; old_params = None\n\n# -------- Training loop with epoch/task checks --------\nmetrics = {\"task\":[], \"epoch\":[], \"mean_auc\":[], \"skipped_batches\":[], \"avg_loss\":[]}\nstart = time.time()\nfor task_id, task_recs in enumerate(task_recs_list):\n    print(f\"\\n=== TASK {task_id+1}/{len(task_recs_list)} | samples: {len(task_recs)} ===\")\n    train_ds = CXRRecordsDataset(task_recs, image_col=\"resolved_path\", image_size=IMAGE_SIZE)\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n\n    # optional: unfreeze backbone after first task\n    if task_id == 1:\n        for p in model.parameters(): p.requires_grad = True\n        optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-2)\n\n    for epoch in range(1, EPOCHS+1):\n        model.train()\n        running_loss = 0.0; step = 0; skipped = 0\n        for imgs, labels, masks, _ in train_loader:\n            step += 1\n            try:\n                imgs = imgs.to(DEVICE); labels = labels.to(DEVICE); masks = masks.to(DEVICE)\n            except Exception as e:\n                print(\"[WARN] device/transform issue:\", e); skipped += 1; continue\n            if masks.sum().item() == 0:\n                skipped += 1\n                if skipped % 50 == 1:\n                    print(f\"[WARN] Skipping batch (no valid labels). skipped so far: {skipped}\")\n                continue\n\n            # exemplar replay\n            ex_t, ex_l = (None, None)\n            if len(exemplar_buffer) > 0:\n                try:\n                    ex_t, ex_l = exemplar_buffer.sample_as_batch(EXEMPLARS_PER_BATCH, DEVICE)\n                except Exception:\n                    ex_t, ex_l = None, None\n            if ex_t is not None:\n                if imgs.shape[2:] != ex_t.shape[2:]:\n                    # resize exemplars if shape mismatch\n                    ex_t = torch.stack([pil_to_tensor_safe(Image.open(p).convert(\"RGB\"), image_size=IMAGE_SIZE) for p in exemplar_buffer.paths[:ex_t.shape[0]]]).to(DEVICE)\n                imgs = torch.cat([imgs, ex_t], dim=0)\n                labels = torch.cat([labels, ex_l], dim=0)\n                masks = torch.cat([masks, torch.ones_like(ex_l)], dim=0)\n\n            optimizer.zero_grad()\n            with autocast():\n                out = model(imgs)\n                if out.shape != labels.shape:\n                    raise RuntimeError(f\"Model output {out.shape} != labels {labels.shape}\")\n                bce = F.binary_cross_entropy_with_logits(out, labels, reduction='none')\n                loss = (bce * masks).sum() / (masks.sum().clamp_min(1.0))\n                if USE_EWC and fisher is not None and old_params is not None:\n                    loss = loss + ewc_penalty(model, fisher, old_params, EWC_LAMBDA)\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer); scaler.update()\n            running_loss += float(loss.item())\n\n            if step % 50 == 0:\n                print(f\"Epoch {epoch} step {step}/{len(train_loader)} loss {running_loss/step:.4f}\")\n\n        avg_loss = running_loss / max(1, step)\n        print(f\"Epoch {epoch} finished. avg loss {avg_loss:.4f}. skipped_batches {skipped}\")\n        # epoch-level checks\n        metrics[\"task\"].append(task_id+1); metrics[\"epoch\"].append(epoch); metrics[\"skipped_batches\"].append(skipped); metrics[\"avg_loss\"].append(avg_loss)\n\n        # small train-batch loss diagnostic\n        try:\n            sample_batch = next(iter(train_loader))\n            t_imgs, t_labels, t_masks, _ = sample_batch\n            with torch.no_grad():\n                t_out = model(t_imgs.to(DEVICE))\n                t_bce = F.binary_cross_entropy_with_logits(t_out, t_labels.to(DEVICE), reduction='none')\n                t_loss = (t_bce * t_masks.to(DEVICE)).sum() / (t_masks.to(DEVICE).sum().clamp_min(1.0))\n            print(\"Sample train-batch loss:\", float(t_loss.item()))\n        except Exception as e:\n            print(\"[WARN] Could not compute sample train-batch loss:\", e)\n\n        # validation AUROC (health check)\n        print(\"Running validation health-check...\")\n        aucs, mean_auc = safe_evaluate_auroc(model, val_loader, DEVICE, NUM_LABELS)\n        print(\"Validation mean AUROC:\", mean_auc)\n        # print per-label presence & auc\n        val_presence = label_presence_stats(val_sample)\n        for i,c in enumerate(label_cols):\n            pres = val_presence[c]\n            print(f\"Label {i:02d} {c[:20]:20s} pos={pres['pos']:5d} zero={pres['zero']:5d} uncertain={pres['uncertain']:5d}  AUC={aucs.get(i,float('nan'))}\")\n        metrics[\"mean_auc\"].append(mean_auc)\n\n    # After task: add exemplars (balanced-ish sampling)\n    k = min(EXEMPLAR_BUDGET // (task_id+1), len(task_recs))\n    if k > 0:\n        sample_recs = random.sample(task_recs, k)\n        paths = [r.get(\"resolved_path\") for r in sample_recs]\n        labs = []\n        for r in sample_recs:\n            lv=[]\n            for c in label_cols:\n                try:\n                    lv.append(float(r.get(c,\"\") if r.get(c,\"\") not in (None,\"\") else 0.0))\n                except:\n                    lv.append(0.0)\n            if len(lv)!=NUM_LABELS: lv=[0.0]*NUM_LABELS\n            labs.append(lv)\n        exemplar_buffer.add(paths, labs)\n    print(\"Exemplar buffer size:\", len(exemplar_buffer))\n\n    # compute fisher for EWC\n    if USE_EWC:\n        if len(exemplar_buffer) > 0:\n            ex_recs = []\n            for p, lab in zip(exemplar_buffer.paths, exemplar_buffer.labels):\n                rec = {\"resolved_path\": p}\n                for i in range(NUM_LABELS): rec[label_cols[i]] = float(lab[i])\n                ex_recs.append(rec)\n            fisher_loader = DataLoader(CXRRecordsDataset(ex_recs, image_col=\"resolved_path\", image_size=IMAGE_SIZE),\n                                       batch_size=min(16,BATCH_SIZE), shuffle=True, num_workers=0)\n        else:\n            fisher_loader = DataLoader(train_ds, batch_size=min(16,BATCH_SIZE), shuffle=True, num_workers=0)\n        print(\"Computing Fisher (EWC)...\")\n        fisher = compute_fisher(model, fisher_loader, DEVICE, samples=FISHER_SAMPLES)\n        old_params = snapshot_params(model)\n\n# Save final artifacts\nimport pandas as pd\npd.DataFrame(metrics).to_csv(os.path.join(SAVE_DIR, \"training_metrics_final.csv\"), index=False)\ntorch.save({\"model_state\": model.state_dict(), \"NUM_LABELS\": NUM_LABELS}, os.path.join(SAVE_DIR, \"final_cl_model.pth\"))\nprint(\"Saved final artifacts to:\", SAVE_DIR)\nprint(\"Total training time (min):\", (time.time() - start)/60.0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T05:42:15.991579Z","iopub.execute_input":"2025-11-09T05:42:15.992226Z","iopub.status.idle":"2025-11-09T09:19:19.194196Z","shell.execute_reply.started":"2025-11-09T05:42:15.992202Z","shell.execute_reply":"2025-11-09T09:19:19.193129Z"}},"outputs":[{"name":"stdout","text":"Device: cuda | batch: 16 img: 224 num_workers: 0\nNUM_LABELS = 14\nRecords loaded from mapped CSV: 220854\nLoaded task manifest with 4 tasks.\n Task 1 samples: 55854\n Task 2 samples: 55854\n Task 3 samples: 55853\n Task 4 samples: 55853\nValidation sample size: 2000\nValidation label presence (pos/zero/uncertain/total):\nNo Finding           {'pos': 199, 'zero': 0, 'uncertain': 0, 'total': 199}\nEnlarged Cardiomedia {'pos': 106, 'zero': 197, 'uncertain': 121, 'total': 424}\nCardiomegaly         {'pos': 241, 'zero': 107, 'uncertain': 75, 'total': 423}\nLung Opacity         {'pos': 948, 'zero': 55, 'uncertain': 61, 'total': 1064}\nLung Lesion          {'pos': 70, 'zero': 14, 'uncertain': 17, 'total': 101}\nEdema                {'pos': 477, 'zero': 187, 'uncertain': 106, 'total': 770}\nConsolidation        {'pos': 143, 'zero': 255, 'uncertain': 218, 'total': 616}\nPneumonia            {'pos': 59, 'zero': 32, 'uncertain': 162, 'total': 253}\nAtelectasis          {'pos': 307, 'zero': 6, 'uncertain': 284, 'total': 597}\nPneumothorax         {'pos': 191, 'zero': 504, 'uncertain': 22, 'total': 717}\nPleural Effusion     {'pos': 752, 'zero': 310, 'uncertain': 89, 'total': 1151}\nPleural Other        {'pos': 28, 'zero': 1, 'uncertain': 24, 'total': 53}\nFracture             {'pos': 93, 'zero': 23, 'uncertain': 3, 'total': 119}\nSupport Devices      {'pos': 1016, 'zero': 60, 'uncertain': 11, 'total': 1087}\n\n=== TASK 1/4 | samples: 55854 ===\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/99983984.py:299: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n/tmp/ipykernel_48/99983984.py:347: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 step 50/3491 loss 0.3644\nEpoch 1 step 100/3491 loss 0.2303\nEpoch 1 step 150/3491 loss 0.1692\nEpoch 1 step 200/3491 loss 0.1342\nEpoch 1 step 250/3491 loss 0.1114\nEpoch 1 step 300/3491 loss 0.0954\nEpoch 1 step 350/3491 loss 0.0835\nEpoch 1 step 400/3491 loss 0.0743\nEpoch 1 step 450/3491 loss 0.0670\nEpoch 1 step 500/3491 loss 0.0609\nEpoch 1 step 550/3491 loss 0.0559\nEpoch 1 step 600/3491 loss 0.0517\nEpoch 1 step 650/3491 loss 0.0481\nEpoch 1 step 700/3491 loss 0.0449\nEpoch 1 step 750/3491 loss 0.0422\nEpoch 1 step 800/3491 loss 0.0398\nEpoch 1 step 850/3491 loss 0.0376\nEpoch 1 step 900/3491 loss 0.0356\nEpoch 1 step 950/3491 loss 0.0339\nEpoch 1 step 1000/3491 loss 0.0323\nEpoch 1 step 1050/3491 loss 0.0309\nEpoch 1 step 1100/3491 loss 0.0296\nEpoch 1 step 1150/3491 loss 0.0284\nEpoch 1 step 1200/3491 loss 0.0272\nEpoch 1 step 1250/3491 loss 0.0262\nEpoch 1 step 1300/3491 loss 0.0253\nEpoch 1 step 1350/3491 loss 0.0244\nEpoch 1 step 1400/3491 loss 0.0236\nEpoch 1 step 1450/3491 loss 0.0228\nEpoch 1 step 1500/3491 loss 0.0221\nEpoch 1 step 1550/3491 loss 0.0214\nEpoch 1 step 1600/3491 loss 0.0207\nEpoch 1 step 1650/3491 loss 0.0201\nEpoch 1 step 1700/3491 loss 0.0196\nEpoch 1 step 1750/3491 loss 0.0190\nEpoch 1 step 1800/3491 loss 0.0185\nEpoch 1 step 1850/3491 loss 0.0181\nEpoch 1 step 1900/3491 loss 0.0176\nEpoch 1 step 1950/3491 loss 0.0172\nEpoch 1 step 2000/3491 loss 0.0168\nEpoch 1 step 2050/3491 loss 0.0164\nEpoch 1 step 2100/3491 loss 0.0160\nEpoch 1 step 2150/3491 loss 0.0156\nEpoch 1 step 2200/3491 loss 0.0153\nEpoch 1 step 2250/3491 loss 0.0150\nEpoch 1 step 2300/3491 loss 0.0146\nEpoch 1 step 2350/3491 loss 0.0143\nEpoch 1 step 2400/3491 loss 0.0141\nEpoch 1 step 2450/3491 loss 0.0138\nEpoch 1 step 2500/3491 loss 0.0135\nEpoch 1 step 2550/3491 loss 0.0133\nEpoch 1 step 2600/3491 loss 0.0130\nEpoch 1 step 2650/3491 loss 0.0128\nEpoch 1 step 2700/3491 loss 0.0125\nEpoch 1 step 2750/3491 loss 0.0123\nEpoch 1 step 2800/3491 loss 0.0121\nEpoch 1 step 2850/3491 loss 0.0119\nEpoch 1 step 2900/3491 loss 0.0117\nEpoch 1 step 2950/3491 loss 0.0115\nEpoch 1 step 3000/3491 loss 0.0113\nEpoch 1 step 3050/3491 loss 0.0111\nEpoch 1 step 3100/3491 loss 0.0110\nEpoch 1 step 3150/3491 loss 0.0108\nEpoch 1 step 3200/3491 loss 0.0106\nEpoch 1 step 3250/3491 loss 0.0105\nEpoch 1 step 3300/3491 loss 0.0103\nEpoch 1 step 3350/3491 loss 0.0102\nEpoch 1 step 3400/3491 loss 0.0100\nEpoch 1 step 3450/3491 loss 0.0099\nEpoch 1 finished. avg loss 0.0098. skipped_batches 0\nSample train-batch loss: 0.00022704048024024814\nRunning validation health-check...\nValidation mean AUROC: 0.5043624585619717\nLabel 00 No Finding           pos=  199 zero=    0 uncertain=    0  AUC=0.4715554451881841\nLabel 01 Enlarged Cardiomedia pos=  106 zero=  197 uncertain=  121  AUC=0.4878763124863023\nLabel 02 Cardiomegaly         pos=  241 zero=  107 uncertain=   75  AUC=0.5090005401975377\nLabel 03 Lung Opacity         pos=  948 zero=   55 uncertain=   61  AUC=0.5057650888001155\nLabel 04 Lung Lesion          pos=   70 zero=   14 uncertain=   17  AUC=0.4879052553663953\nLabel 05 Edema                pos=  477 zero=  187 uncertain=  106  AUC=0.5595570917490168\nLabel 06 Consolidation        pos=  143 zero=  255 uncertain=  218  AUC=0.5383335027923073\nLabel 07 Pneumonia            pos=   59 zero=   32 uncertain=  162  AUC=0.49091417144753274\nLabel 08 Atelectasis          pos=  307 zero=    6 uncertain=  284  AUC=0.5346848779511728\nLabel 09 Pneumothorax         pos=  191 zero=  504 uncertain=   22  AUC=0.4628804783528547\nLabel 10 Pleural Effusion     pos=  752 zero=  310 uncertain=   89  AUC=0.5398627165166394\nLabel 11 Pleural Other        pos=   28 zero=    1 uncertain=   24  AUC=0.4394378441031585\nLabel 12 Fracture             pos=   93 zero=   23 uncertain=    3  AUC=0.48753037761275664\nLabel 13 Support Devices      pos= 1016 zero=   60 uncertain=   11  AUC=0.5457707173036297\nEpoch 2 step 50/3491 loss 0.0002\nEpoch 2 step 100/3491 loss 0.0002\nEpoch 2 step 150/3491 loss 0.0002\nEpoch 2 step 200/3491 loss 0.0002\nEpoch 2 step 250/3491 loss 0.0002\nEpoch 2 step 300/3491 loss 0.0002\nEpoch 2 step 350/3491 loss 0.0002\nEpoch 2 step 400/3491 loss 0.0002\nEpoch 2 step 450/3491 loss 0.0002\nEpoch 2 step 500/3491 loss 0.0002\nEpoch 2 step 550/3491 loss 0.0002\nEpoch 2 step 600/3491 loss 0.0002\nEpoch 2 step 650/3491 loss 0.0002\nEpoch 2 step 700/3491 loss 0.0002\nEpoch 2 step 750/3491 loss 0.0002\nEpoch 2 step 800/3491 loss 0.0002\nEpoch 2 step 850/3491 loss 0.0002\nEpoch 2 step 900/3491 loss 0.0002\nEpoch 2 step 950/3491 loss 0.0002\nEpoch 2 step 1000/3491 loss 0.0002\nEpoch 2 step 1050/3491 loss 0.0002\nEpoch 2 step 1100/3491 loss 0.0002\nEpoch 2 step 1150/3491 loss 0.0002\nEpoch 2 step 1200/3491 loss 0.0002\nEpoch 2 step 1250/3491 loss 0.0001\nEpoch 2 step 1300/3491 loss 0.0001\nEpoch 2 step 1350/3491 loss 0.0001\nEpoch 2 step 1400/3491 loss 0.0001\nEpoch 2 step 1450/3491 loss 0.0001\nEpoch 2 step 1500/3491 loss 0.0001\nEpoch 2 step 1550/3491 loss 0.0001\nEpoch 2 step 1600/3491 loss 0.0001\nEpoch 2 step 1650/3491 loss 0.0001\nEpoch 2 step 1700/3491 loss 0.0001\nEpoch 2 step 1750/3491 loss 0.0001\nEpoch 2 step 1800/3491 loss 0.0001\nEpoch 2 step 1850/3491 loss 0.0001\nEpoch 2 step 1900/3491 loss 0.0001\nEpoch 2 step 1950/3491 loss 0.0001\nEpoch 2 step 2000/3491 loss 0.0001\nEpoch 2 step 2050/3491 loss 0.0001\nEpoch 2 step 2100/3491 loss 0.0001\nEpoch 2 step 2150/3491 loss 0.0001\nEpoch 2 step 2200/3491 loss 0.0001\nEpoch 2 step 2250/3491 loss 0.0001\nEpoch 2 step 2300/3491 loss 0.0001\nEpoch 2 step 2350/3491 loss 0.0001\nEpoch 2 step 2400/3491 loss 0.0001\nEpoch 2 step 2450/3491 loss 0.0001\nEpoch 2 step 2500/3491 loss 0.0001\nEpoch 2 step 2550/3491 loss 0.0001\nEpoch 2 step 2600/3491 loss 0.0001\nEpoch 2 step 2650/3491 loss 0.0001\nEpoch 2 step 2700/3491 loss 0.0001\nEpoch 2 step 2750/3491 loss 0.0001\nEpoch 2 step 2800/3491 loss 0.0001\nEpoch 2 step 2850/3491 loss 0.0001\nEpoch 2 step 2900/3491 loss 0.0001\nEpoch 2 step 2950/3491 loss 0.0001\nEpoch 2 step 3000/3491 loss 0.0001\nEpoch 2 step 3050/3491 loss 0.0001\nEpoch 2 step 3100/3491 loss 0.0001\nEpoch 2 step 3150/3491 loss 0.0001\nEpoch 2 step 3200/3491 loss 0.0001\nEpoch 2 step 3250/3491 loss 0.0001\nEpoch 2 step 3300/3491 loss 0.0001\nEpoch 2 step 3350/3491 loss 0.0001\nEpoch 2 step 3400/3491 loss 0.0001\nEpoch 2 step 3450/3491 loss 0.0001\nEpoch 2 finished. avg loss 0.0001. skipped_batches 0\nSample train-batch loss: 2.4054732421063818e-05\nRunning validation health-check...\nValidation mean AUROC: 0.506288692764781\nLabel 00 No Finding           pos=  199 zero=    0 uncertain=    0  AUC=0.4687959508815594\nLabel 01 Enlarged Cardiomedia pos=  106 zero=  197 uncertain=  121  AUC=0.4880904943117292\nLabel 02 Cardiomegaly         pos=  241 zero=  107 uncertain=   75  AUC=0.510529134103449\nLabel 03 Lung Opacity         pos=  948 zero=   55 uncertain=   61  AUC=0.5101113410662431\nLabel 04 Lung Lesion          pos=   70 zero=   14 uncertain=   17  AUC=0.49052553663952625\nLabel 05 Edema                pos=  477 zero=  187 uncertain=  106  AUC=0.5648346596078853\nLabel 06 Consolidation        pos=  143 zero=  255 uncertain=  218  AUC=0.5378514861552018\nLabel 07 Pneumonia            pos=   59 zero=   32 uncertain=  162  AUC=0.49101895755289515\nLabel 08 Atelectasis          pos=  307 zero=    6 uncertain=  284  AUC=0.5360268667111752\nLabel 09 Pneumothorax         pos=  191 zero=  504 uncertain=   22  AUC=0.46875569795004035\nLabel 10 Pleural Effusion     pos=  752 zero=  310 uncertain=   89  AUC=0.5379021327741408\nLabel 11 Pleural Other        pos=   28 zero=    1 uncertain=   24  AUC=0.444943494639235\nLabel 12 Fracture             pos=   93 zero=   23 uncertain=    3  AUC=0.49195098984499663\nLabel 13 Support Devices      pos= 1016 zero=   60 uncertain=   11  AUC=0.546704956468856\nExemplar buffer size: 1200\nComputing Fisher (EWC)...\n\n=== TASK 2/4 | samples: 55854 ===\nEpoch 1 step 50/3491 loss 0.0000\nEpoch 1 step 100/3491 loss 0.0000\nEpoch 1 step 150/3491 loss 0.0000\nEpoch 1 step 200/3491 loss 0.0000\nEpoch 1 step 250/3491 loss 0.0000\nEpoch 1 step 300/3491 loss 0.0000\nEpoch 1 step 350/3491 loss 0.0000\nEpoch 1 step 400/3491 loss 0.0000\nEpoch 1 step 450/3491 loss 0.0000\nEpoch 1 step 500/3491 loss 0.0000\nEpoch 1 step 550/3491 loss 0.0000\nEpoch 1 step 600/3491 loss 0.0000\nEpoch 1 step 650/3491 loss 0.0000\nEpoch 1 step 700/3491 loss 0.0000\nEpoch 1 step 750/3491 loss 0.0000\nEpoch 1 step 800/3491 loss 0.0000\nEpoch 1 step 850/3491 loss 0.0000\nEpoch 1 step 900/3491 loss 0.0000\nEpoch 1 step 950/3491 loss 0.0000\nEpoch 1 step 1000/3491 loss 0.0000\nEpoch 1 step 1050/3491 loss 0.0000\nEpoch 1 step 1100/3491 loss 0.0000\nEpoch 1 step 1150/3491 loss 0.0000\nEpoch 1 step 1200/3491 loss 0.0000\nEpoch 1 step 1250/3491 loss 0.0000\nEpoch 1 step 1300/3491 loss 0.0000\nEpoch 1 step 1350/3491 loss 0.0000\nEpoch 1 step 1400/3491 loss 0.0000\nEpoch 1 step 1450/3491 loss 0.0000\nEpoch 1 step 1500/3491 loss 0.0000\nEpoch 1 step 1550/3491 loss 0.0000\nEpoch 1 step 1600/3491 loss 0.0000\nEpoch 1 step 1650/3491 loss 0.0000\nEpoch 1 step 1700/3491 loss 0.0000\nEpoch 1 step 1750/3491 loss 0.0000\nEpoch 1 step 1800/3491 loss 0.0000\nEpoch 1 step 1850/3491 loss 0.0000\nEpoch 1 step 1900/3491 loss 0.0000\nEpoch 1 step 1950/3491 loss 0.0000\nEpoch 1 step 2000/3491 loss 0.0000\nEpoch 1 step 2050/3491 loss 0.0000\nEpoch 1 step 2100/3491 loss 0.0000\nEpoch 1 step 2150/3491 loss 0.0000\nEpoch 1 step 2200/3491 loss 0.0000\nEpoch 1 step 2250/3491 loss 0.0000\nEpoch 1 step 2300/3491 loss 0.0000\nEpoch 1 step 2350/3491 loss 0.0000\nEpoch 1 step 2400/3491 loss 0.0000\nEpoch 1 step 2450/3491 loss 0.0000\nEpoch 1 step 2500/3491 loss 0.0000\nEpoch 1 step 2550/3491 loss 0.0000\nEpoch 1 step 2600/3491 loss 0.0000\nEpoch 1 step 2650/3491 loss 0.0000\nEpoch 1 step 2700/3491 loss 0.0000\nEpoch 1 step 2750/3491 loss 0.0000\nEpoch 1 step 2800/3491 loss 0.0000\nEpoch 1 step 2850/3491 loss 0.0000\nEpoch 1 step 2900/3491 loss 0.0000\nEpoch 1 step 2950/3491 loss 0.0000\nEpoch 1 step 3000/3491 loss 0.0000\nEpoch 1 step 3050/3491 loss 0.0000\nEpoch 1 step 3100/3491 loss 0.0000\nEpoch 1 step 3150/3491 loss 0.0000\nEpoch 1 step 3200/3491 loss 0.0000\nEpoch 1 step 3250/3491 loss 0.0000\nEpoch 1 step 3300/3491 loss 0.0000\nEpoch 1 step 3350/3491 loss 0.0000\nEpoch 1 step 3400/3491 loss 0.0000\nEpoch 1 step 3450/3491 loss 0.0000\nEpoch 1 finished. avg loss 0.0000. skipped_batches 0\nSample train-batch loss: 0.0\nRunning validation health-check...\nValidation mean AUROC: 0.5130806280267047\nLabel 00 No Finding           pos=  199 zero=    0 uncertain=    0  AUC=0.5086314414939774\nLabel 01 Enlarged Cardiomedia pos=  106 zero=  197 uncertain=  121  AUC=0.49230439720268576\nLabel 02 Cardiomegaly         pos=  241 zero=  107 uncertain=   75  AUC=0.49300927771579006\nLabel 03 Lung Opacity         pos=  948 zero=   55 uncertain=   61  AUC=0.5117853676340826\nLabel 04 Lung Lesion          pos=   70 zero=   14 uncertain=   17  AUC=0.5863138415988157\nLabel 05 Edema                pos=  477 zero=  187 uncertain=  106  AUC=0.5207544416776444\nLabel 06 Consolidation        pos=  143 zero=  255 uncertain=  218  AUC=0.5266069417927253\nLabel 07 Pneumonia            pos=   59 zero=   32 uncertain=  162  AUC=0.5589203538277491\nLabel 08 Atelectasis          pos=  307 zero=    6 uncertain=  284  AUC=0.5449946224249689\nLabel 09 Pneumothorax         pos=  191 zero=  504 uncertain=   22  AUC=0.51321056150313\nLabel 10 Pleural Effusion     pos=  752 zero=  310 uncertain=   89  AUC=0.4900015556805783\nLabel 11 Pleural Other        pos=   28 zero=    1 uncertain=   24  AUC=0.42299333526514055\nLabel 12 Fracture             pos=   93 zero=   23 uncertain=    3  AUC=0.512232803874802\nLabel 13 Support Devices      pos= 1016 zero=   60 uncertain=   11  AUC=0.5013698506817745\nEpoch 2 step 50/3491 loss 0.0000\nEpoch 2 step 100/3491 loss 0.0000\nEpoch 2 step 150/3491 loss 0.0000\nEpoch 2 step 200/3491 loss 0.0000\nEpoch 2 step 250/3491 loss 0.0000\nEpoch 2 step 300/3491 loss 0.0000\nEpoch 2 step 350/3491 loss 0.0000\nEpoch 2 step 400/3491 loss 0.0000\nEpoch 2 step 450/3491 loss 0.0000\nEpoch 2 step 500/3491 loss 0.0000\nEpoch 2 step 550/3491 loss 0.0000\nEpoch 2 step 600/3491 loss 0.0000\nEpoch 2 step 650/3491 loss 0.0000\nEpoch 2 step 700/3491 loss 0.0000\nEpoch 2 step 750/3491 loss 0.0000\nEpoch 2 step 800/3491 loss 0.0000\nEpoch 2 step 850/3491 loss 0.0000\nEpoch 2 step 900/3491 loss 0.0000\nEpoch 2 step 950/3491 loss 0.0000\nEpoch 2 step 1000/3491 loss 0.0000\nEpoch 2 step 1050/3491 loss 0.0000\nEpoch 2 step 1100/3491 loss 0.0000\nEpoch 2 step 1150/3491 loss 0.0000\nEpoch 2 step 1200/3491 loss 0.0000\nEpoch 2 step 1250/3491 loss 0.0000\nEpoch 2 step 1300/3491 loss 0.0000\nEpoch 2 step 1350/3491 loss 0.0000\nEpoch 2 step 1400/3491 loss 0.0000\nEpoch 2 step 1450/3491 loss 0.0000\nEpoch 2 step 1500/3491 loss 0.0000\nEpoch 2 step 1550/3491 loss 0.0000\nEpoch 2 step 1600/3491 loss 0.0000\nEpoch 2 step 1650/3491 loss 0.0000\nEpoch 2 step 1700/3491 loss 0.0000\nEpoch 2 step 1750/3491 loss 0.0000\nEpoch 2 step 1800/3491 loss 0.0000\nEpoch 2 step 1850/3491 loss 0.0000\nEpoch 2 step 1900/3491 loss 0.0000\nEpoch 2 step 1950/3491 loss 0.0000\nEpoch 2 step 2000/3491 loss 0.0000\nEpoch 2 step 2050/3491 loss 0.0000\nEpoch 2 step 2100/3491 loss 0.0000\nEpoch 2 step 2150/3491 loss 0.0000\nEpoch 2 step 2200/3491 loss 0.0000\nEpoch 2 step 2250/3491 loss 0.0000\nEpoch 2 step 2300/3491 loss 0.0000\nEpoch 2 step 2350/3491 loss 0.0000\nEpoch 2 step 2400/3491 loss 0.0000\nEpoch 2 step 2450/3491 loss 0.0000\nEpoch 2 step 2500/3491 loss 0.0000\nEpoch 2 step 2550/3491 loss 0.0000\nEpoch 2 step 2600/3491 loss 0.0000\nEpoch 2 step 2650/3491 loss 0.0000\nEpoch 2 step 2700/3491 loss 0.0000\nEpoch 2 step 2750/3491 loss 0.0000\nEpoch 2 step 2800/3491 loss 0.0000\nEpoch 2 step 2850/3491 loss 0.0000\nEpoch 2 step 2900/3491 loss 0.0000\nEpoch 2 step 2950/3491 loss 0.0000\nEpoch 2 step 3000/3491 loss 0.0000\nEpoch 2 step 3050/3491 loss 0.0000\nEpoch 2 step 3100/3491 loss 0.0000\nEpoch 2 step 3150/3491 loss 0.0000\nEpoch 2 step 3200/3491 loss 0.0000\nEpoch 2 step 3250/3491 loss 0.0000\nEpoch 2 step 3300/3491 loss 0.0000\nEpoch 2 step 3350/3491 loss 0.0000\nEpoch 2 step 3400/3491 loss 0.0000\nEpoch 2 step 3450/3491 loss 0.0000\nEpoch 2 finished. avg loss 0.0000. skipped_batches 0\nSample train-batch loss: 0.0\nRunning validation health-check...\nValidation mean AUROC: 0.505898234686236\nLabel 00 No Finding           pos=  199 zero=    0 uncertain=    0  AUC=0.5192815828169164\nLabel 01 Enlarged Cardiomedia pos=  106 zero=  197 uncertain=  121  AUC=0.48840927656352734\nLabel 02 Cardiomegaly         pos=  241 zero=  107 uncertain=   75  AUC=0.4700013445964913\nLabel 03 Lung Opacity         pos=  948 zero=   55 uncertain=   61  AUC=0.49123479889621535\nLabel 04 Lung Lesion          pos=   70 zero=   14 uncertain=   17  AUC=0.5865136935603257\nLabel 05 Edema                pos=  477 zero=  187 uncertain=  106  AUC=0.49549397016536106\nLabel 06 Consolidation        pos=  143 zero=  255 uncertain=  218  AUC=0.5061155860832759\nLabel 07 Pneumonia            pos=   59 zero=   32 uncertain=  162  AUC=0.5695124826448013\nLabel 08 Atelectasis          pos=  307 zero=    6 uncertain=  284  AUC=0.5353015193813961\nLabel 09 Pneumothorax         pos=  191 zero=  504 uncertain=   22  AUC=0.4997916178270949\nLabel 10 Pleural Effusion     pos=  752 zero=  310 uncertain=   89  AUC=0.47139359144844517\nLabel 11 Pleural Other        pos=   28 zero=    1 uncertain=   24  AUC=0.4668574326282237\nLabel 12 Fracture             pos=   93 zero=   23 uncertain=    3  AUC=0.4902143207537595\nLabel 13 Support Devices      pos= 1016 zero=   60 uncertain=   11  AUC=0.4924540682414698\nExemplar buffer size: 1200\nComputing Fisher (EWC)...\n\n=== TASK 3/4 | samples: 55853 ===\nEpoch 1 step 50/3491 loss 0.0000\nEpoch 1 step 100/3491 loss 0.0000\nEpoch 1 step 150/3491 loss 0.0000\nEpoch 1 step 200/3491 loss 0.0000\nEpoch 1 step 250/3491 loss 0.0000\nEpoch 1 step 300/3491 loss 0.0000\nEpoch 1 step 350/3491 loss 0.0000\nEpoch 1 step 400/3491 loss 0.0000\nEpoch 1 step 450/3491 loss 0.0000\nEpoch 1 step 500/3491 loss 0.0000\nEpoch 1 step 550/3491 loss 0.0000\nEpoch 1 step 600/3491 loss 0.0000\nEpoch 1 step 650/3491 loss 0.0000\nEpoch 1 step 700/3491 loss 0.0000\nEpoch 1 step 750/3491 loss 0.0000\nEpoch 1 step 800/3491 loss 0.0000\nEpoch 1 step 850/3491 loss 0.0000\nEpoch 1 step 900/3491 loss 0.0000\nEpoch 1 step 950/3491 loss 0.0000\nEpoch 1 step 1000/3491 loss 0.0000\nEpoch 1 step 1050/3491 loss 0.0000\nEpoch 1 step 1100/3491 loss 0.0000\nEpoch 1 step 1150/3491 loss 0.0000\nEpoch 1 step 1200/3491 loss 0.0000\nEpoch 1 step 1250/3491 loss 0.0000\nEpoch 1 step 1300/3491 loss 0.0000\nEpoch 1 step 1350/3491 loss 0.0000\nEpoch 1 step 1400/3491 loss 0.0000\nEpoch 1 step 1450/3491 loss 0.0000\nEpoch 1 step 1500/3491 loss 0.0000\nEpoch 1 step 1550/3491 loss 0.0000\nEpoch 1 step 1600/3491 loss 0.0000\nEpoch 1 step 1650/3491 loss 0.0000\nEpoch 1 step 1700/3491 loss 0.0000\nEpoch 1 step 1750/3491 loss 0.0000\nEpoch 1 step 1800/3491 loss 0.0000\nEpoch 1 step 1850/3491 loss 0.0000\nEpoch 1 step 1900/3491 loss 0.0000\nEpoch 1 step 1950/3491 loss 0.0000\nEpoch 1 step 2000/3491 loss 0.0000\nEpoch 1 step 2050/3491 loss 0.0000\nEpoch 1 step 2100/3491 loss 0.0000\nEpoch 1 step 2150/3491 loss 0.0000\nEpoch 1 step 2200/3491 loss 0.0000\nEpoch 1 step 2250/3491 loss 0.0000\nEpoch 1 step 2300/3491 loss 0.0000\nEpoch 1 step 2350/3491 loss 0.0000\nEpoch 1 step 2400/3491 loss 0.0000\nEpoch 1 step 2450/3491 loss 0.0000\nEpoch 1 step 2500/3491 loss 0.0000\nEpoch 1 step 2550/3491 loss 0.0000\nEpoch 1 step 2600/3491 loss 0.0000\nEpoch 1 step 2650/3491 loss 0.0000\nEpoch 1 step 2700/3491 loss 0.0000\nEpoch 1 step 2750/3491 loss 0.0000\nEpoch 1 step 2800/3491 loss 0.0000\nEpoch 1 step 2850/3491 loss 0.0000\nEpoch 1 step 2900/3491 loss 0.0000\nEpoch 1 step 2950/3491 loss 0.0000\nEpoch 1 step 3000/3491 loss 0.0000\nEpoch 1 step 3050/3491 loss 0.0000\nEpoch 1 step 3100/3491 loss 0.0000\nEpoch 1 step 3150/3491 loss 0.0000\nEpoch 1 step 3200/3491 loss 0.0000\nEpoch 1 step 3250/3491 loss 0.0000\nEpoch 1 step 3300/3491 loss 0.0000\nEpoch 1 step 3350/3491 loss 0.0000\nEpoch 1 step 3400/3491 loss 0.0000\nEpoch 1 step 3450/3491 loss 0.0000\nEpoch 1 finished. avg loss 0.0000. skipped_batches 0\nSample train-batch loss: 0.0\nRunning validation health-check...\nValidation mean AUROC: 0.5072305713415535\nLabel 00 No Finding           pos=  199 zero=    0 uncertain=    0  AUC=0.5224916922201234\nLabel 01 Enlarged Cardiomedia pos=  106 zero=  197 uncertain=  121  AUC=0.4956018011197227\nLabel 02 Cardiomegaly         pos=  241 zero=  107 uncertain=   75  AUC=0.4651702329926236\nLabel 03 Lung Opacity         pos=  948 zero=   55 uncertain=   61  AUC=0.4871878559625227\nLabel 04 Lung Lesion          pos=   70 zero=   14 uncertain=   17  AUC=0.5901369356032569\nLabel 05 Edema                pos=  477 zero=  187 uncertain=  106  AUC=0.4898564429963481\nLabel 06 Consolidation        pos=  143 zero=  255 uncertain=  218  AUC=0.5084993089839617\nLabel 07 Pneumonia            pos=   59 zero=   32 uncertain=  162  AUC=0.569398964363992\nLabel 08 Atelectasis          pos=  307 zero=    6 uncertain=  284  AUC=0.5318854605378345\nLabel 09 Pneumothorax         pos=  191 zero=  504 uncertain=   22  AUC=0.4970450250203317\nLabel 10 Pleural Effusion     pos=  752 zero=  310 uncertain=   89  AUC=0.4750467769708129\nLabel 11 Pleural Other        pos=   28 zero=    1 uncertain=   24  AUC=0.49223956824108955\nLabel 12 Fracture             pos=   93 zero=   23 uncertain=    3  AUC=0.4845109415791284\nLabel 13 Support Devices      pos= 1016 zero=   60 uncertain=   11  AUC=0.49215699219000064\nEpoch 2 step 50/3491 loss 0.0000\nEpoch 2 step 100/3491 loss 0.0000\nEpoch 2 step 150/3491 loss 0.0000\nEpoch 2 step 200/3491 loss 0.0000\nEpoch 2 step 250/3491 loss 0.0000\nEpoch 2 step 300/3491 loss 0.0000\nEpoch 2 step 350/3491 loss 0.0000\nEpoch 2 step 400/3491 loss 0.0000\nEpoch 2 step 450/3491 loss 0.0000\nEpoch 2 step 500/3491 loss 0.0000\nEpoch 2 step 550/3491 loss 0.0000\nEpoch 2 step 600/3491 loss 0.0000\nEpoch 2 step 650/3491 loss 0.0000\nEpoch 2 step 700/3491 loss 0.0000\nEpoch 2 step 750/3491 loss 0.0000\nEpoch 2 step 800/3491 loss 0.0000\nEpoch 2 step 850/3491 loss 0.0000\nEpoch 2 step 900/3491 loss 0.0000\nEpoch 2 step 950/3491 loss 0.0000\nEpoch 2 step 1000/3491 loss 0.0000\nEpoch 2 step 1050/3491 loss 0.0000\nEpoch 2 step 1100/3491 loss 0.0000\nEpoch 2 step 1150/3491 loss 0.0000\nEpoch 2 step 1200/3491 loss 0.0000\nEpoch 2 step 1250/3491 loss 0.0000\nEpoch 2 step 1300/3491 loss 0.0000\nEpoch 2 step 1350/3491 loss 0.0000\nEpoch 2 step 1400/3491 loss 0.0000\nEpoch 2 step 1450/3491 loss 0.0000\nEpoch 2 step 1500/3491 loss 0.0000\nEpoch 2 step 1550/3491 loss 0.0000\nEpoch 2 step 1600/3491 loss 0.0000\nEpoch 2 step 1650/3491 loss 0.0000\nEpoch 2 step 1700/3491 loss 0.0000\nEpoch 2 step 1750/3491 loss 0.0000\nEpoch 2 step 1800/3491 loss 0.0000\nEpoch 2 step 1850/3491 loss 0.0000\nEpoch 2 step 1900/3491 loss 0.0000\nEpoch 2 step 1950/3491 loss 0.0000\nEpoch 2 step 2000/3491 loss 0.0000\nEpoch 2 step 2050/3491 loss 0.0000\nEpoch 2 step 2100/3491 loss 0.0000\nEpoch 2 step 2150/3491 loss 0.0000\nEpoch 2 step 2200/3491 loss 0.0000\nEpoch 2 step 2250/3491 loss 0.0000\nEpoch 2 step 2300/3491 loss 0.0000\nEpoch 2 step 2350/3491 loss 0.0000\nEpoch 2 step 2400/3491 loss 0.0000\nEpoch 2 step 2450/3491 loss 0.0000\nEpoch 2 step 2500/3491 loss 0.0000\nEpoch 2 step 2550/3491 loss 0.0000\nEpoch 2 step 2600/3491 loss 0.0000\nEpoch 2 step 2650/3491 loss 0.0000\nEpoch 2 step 2700/3491 loss 0.0000\nEpoch 2 step 2750/3491 loss 0.0000\nEpoch 2 step 2800/3491 loss 0.0000\nEpoch 2 step 2850/3491 loss 0.0000\nEpoch 2 step 2900/3491 loss 0.0000\nEpoch 2 step 2950/3491 loss 0.0000\nEpoch 2 step 3000/3491 loss 0.0000\nEpoch 2 step 3050/3491 loss 0.0000\nEpoch 2 step 3100/3491 loss 0.0000\nEpoch 2 step 3150/3491 loss 0.0000\nEpoch 2 step 3200/3491 loss 0.0000\nEpoch 2 step 3250/3491 loss 0.0000\nEpoch 2 step 3300/3491 loss 0.0000\nEpoch 2 step 3350/3491 loss 0.0000\nEpoch 2 step 3400/3491 loss 0.0000\nEpoch 2 step 3450/3491 loss 0.0000\nEpoch 2 finished. avg loss 0.0000. skipped_batches 0\nSample train-batch loss: 0.0\nRunning validation health-check...\nValidation mean AUROC: 0.5052929053843476\nLabel 00 No Finding           pos=  199 zero=    0 uncertain=    0  AUC=0.5152162812954277\nLabel 01 Enlarged Cardiomedia pos=  106 zero=  197 uncertain=  121  AUC=0.48994092566396363\nLabel 02 Cardiomegaly         pos=  241 zero=  107 uncertain=   75  AUC=0.470731436901861\nLabel 03 Lung Opacity         pos=  948 zero=   55 uncertain=   61  AUC=0.48643181161861676\nLabel 04 Lung Lesion          pos=   70 zero=   14 uncertain=   17  AUC=0.5827239082161362\nLabel 05 Edema                pos=  477 zero=  187 uncertain=  106  AUC=0.4967293945663351\nLabel 06 Consolidation        pos=  143 zero=  255 uncertain=  218  AUC=0.5055789659989983\nLabel 07 Pneumonia            pos=   59 zero=   32 uncertain=  162  AUC=0.5639151581833582\nLabel 08 Atelectasis          pos=  307 zero=    6 uncertain=  284  AUC=0.5353034433796183\nLabel 09 Pneumothorax         pos=  191 zero=  504 uncertain=   22  AUC=0.4911321808641493\nLabel 10 Pleural Effusion     pos=  752 zero=  310 uncertain=   89  AUC=0.47698605002045824\nLabel 11 Pleural Other        pos=   28 zero=    1 uncertain=   24  AUC=0.4884815995363663\nLabel 12 Fracture             pos=   93 zero=   23 uncertain=    3  AUC=0.47649294337218284\nLabel 13 Support Devices      pos= 1016 zero=   60 uncertain=   11  AUC=0.49443657576339545\nExemplar buffer size: 1200\nComputing Fisher (EWC)...\n\n=== TASK 4/4 | samples: 55853 ===\nEpoch 1 step 50/3491 loss 0.0000\nEpoch 1 step 100/3491 loss 0.0000\nEpoch 1 step 150/3491 loss 0.0000\nEpoch 1 step 200/3491 loss 0.0000\nEpoch 1 step 250/3491 loss 0.0000\nEpoch 1 step 300/3491 loss 0.0000\nEpoch 1 step 350/3491 loss 0.0000\nEpoch 1 step 400/3491 loss 0.0000\nEpoch 1 step 450/3491 loss 0.0000\nEpoch 1 step 500/3491 loss 0.0000\nEpoch 1 step 550/3491 loss 0.0000\nEpoch 1 step 600/3491 loss 0.0000\nEpoch 1 step 650/3491 loss 0.0000\nEpoch 1 step 700/3491 loss 0.0000\nEpoch 1 step 750/3491 loss 0.0000\nEpoch 1 step 800/3491 loss 0.0000\nEpoch 1 step 850/3491 loss 0.0000\nEpoch 1 step 900/3491 loss 0.0000\nEpoch 1 step 950/3491 loss 0.0000\nEpoch 1 step 1000/3491 loss 0.0000\nEpoch 1 step 1050/3491 loss 0.0000\nEpoch 1 step 1100/3491 loss 0.0000\nEpoch 1 step 1150/3491 loss 0.0000\nEpoch 1 step 1200/3491 loss 0.0000\nEpoch 1 step 1250/3491 loss 0.0000\nEpoch 1 step 1300/3491 loss 0.0000\nEpoch 1 step 1350/3491 loss 0.0000\nEpoch 1 step 1400/3491 loss 0.0000\nEpoch 1 step 1450/3491 loss 0.0000\nEpoch 1 step 1500/3491 loss 0.0000\nEpoch 1 step 1550/3491 loss 0.0000\nEpoch 1 step 1600/3491 loss 0.0000\nEpoch 1 step 1650/3491 loss 0.0000\nEpoch 1 step 1700/3491 loss 0.0000\nEpoch 1 step 1750/3491 loss 0.0000\nEpoch 1 step 1800/3491 loss 0.0000\nEpoch 1 step 1850/3491 loss 0.0000\nEpoch 1 step 1900/3491 loss 0.0000\nEpoch 1 step 1950/3491 loss 0.0000\nEpoch 1 step 2000/3491 loss 0.0000\nEpoch 1 step 2050/3491 loss 0.0000\nEpoch 1 step 2100/3491 loss 0.0000\nEpoch 1 step 2150/3491 loss 0.0000\nEpoch 1 step 2200/3491 loss 0.0000\nEpoch 1 step 2250/3491 loss 0.0000\nEpoch 1 step 2300/3491 loss 0.0000\nEpoch 1 step 2350/3491 loss 0.0000\nEpoch 1 step 2400/3491 loss 0.0000\nEpoch 1 step 2450/3491 loss 0.0000\nEpoch 1 step 2500/3491 loss 0.0000\nEpoch 1 step 2550/3491 loss 0.0000\nEpoch 1 step 2600/3491 loss 0.0000\nEpoch 1 step 2650/3491 loss 0.0000\nEpoch 1 step 2700/3491 loss 0.0000\nEpoch 1 step 2750/3491 loss 0.0000\nEpoch 1 step 2800/3491 loss 0.0000\nEpoch 1 step 2850/3491 loss 0.0000\nEpoch 1 step 2900/3491 loss 0.0000\nEpoch 1 step 2950/3491 loss 0.0000\nEpoch 1 step 3000/3491 loss 0.0000\nEpoch 1 step 3050/3491 loss 0.0000\nEpoch 1 step 3100/3491 loss 0.0000\nEpoch 1 step 3150/3491 loss 0.0000\nEpoch 1 step 3200/3491 loss 0.0000\nEpoch 1 step 3250/3491 loss 0.0000\nEpoch 1 step 3300/3491 loss 0.0000\nEpoch 1 step 3350/3491 loss 0.0000\nEpoch 1 step 3400/3491 loss 0.0000\nEpoch 1 step 3450/3491 loss 0.0000\nEpoch 1 finished. avg loss 0.0000. skipped_batches 0\nSample train-batch loss: 0.0\nRunning validation health-check...\nValidation mean AUROC: 0.5055003037945001\nLabel 00 No Finding           pos=  199 zero=    0 uncertain=    0  AUC=0.5153195181906199\nLabel 01 Enlarged Cardiomedia pos=  106 zero=  197 uncertain=  121  AUC=0.49322089617660536\nLabel 02 Cardiomegaly         pos=  241 zero=  107 uncertain=   75  AUC=0.46868859381155364\nLabel 03 Lung Opacity         pos=  948 zero=   55 uncertain=   61  AUC=0.48508517030049253\nLabel 04 Lung Lesion          pos=   70 zero=   14 uncertain=   17  AUC=0.5854441154700222\nLabel 05 Edema                pos=  477 zero=  187 uncertain=  106  AUC=0.493321137388829\nLabel 06 Consolidation        pos=  143 zero=  255 uncertain=  218  AUC=0.5057352448305599\nLabel 07 Pneumonia            pos=   59 zero=   32 uncertain=  162  AUC=0.5631379945685868\nLabel 08 Atelectasis          pos=  307 zero=    6 uncertain=  284  AUC=0.5331783873431701\nLabel 09 Pneumothorax         pos=  191 zero=  504 uncertain=   22  AUC=0.4939482343952142\nLabel 10 Pleural Effusion     pos=  752 zero=  310 uncertain=   89  AUC=0.47557581492089473\nLabel 11 Pleural Other        pos=   28 zero=    1 uncertain=   24  AUC=0.49332620979426256\nLabel 12 Fracture             pos=   93 zero=   23 uncertain=    3  AUC=0.4795321142818479\nLabel 13 Support Devices      pos= 1016 zero=   60 uncertain=   11  AUC=0.49149082165034247\nEpoch 2 step 50/3491 loss 0.0000\nEpoch 2 step 100/3491 loss 0.0000\nEpoch 2 step 150/3491 loss 0.0000\nEpoch 2 step 200/3491 loss 0.0000\nEpoch 2 step 250/3491 loss 0.0000\nEpoch 2 step 300/3491 loss 0.0000\nEpoch 2 step 350/3491 loss 0.0000\nEpoch 2 step 400/3491 loss 0.0000\nEpoch 2 step 450/3491 loss 0.0000\nEpoch 2 step 500/3491 loss 0.0000\nEpoch 2 step 550/3491 loss 0.0000\nEpoch 2 step 600/3491 loss 0.0000\nEpoch 2 step 650/3491 loss 0.0000\nEpoch 2 step 700/3491 loss 0.0000\nEpoch 2 step 750/3491 loss 0.0000\nEpoch 2 step 800/3491 loss 0.0000\nEpoch 2 step 850/3491 loss 0.0000\nEpoch 2 step 900/3491 loss 0.0000\nEpoch 2 step 950/3491 loss 0.0000\nEpoch 2 step 1000/3491 loss 0.0000\nEpoch 2 step 1050/3491 loss 0.0000\nEpoch 2 step 1100/3491 loss 0.0000\nEpoch 2 step 1150/3491 loss 0.0000\nEpoch 2 step 1200/3491 loss 0.0000\nEpoch 2 step 1250/3491 loss 0.0000\nEpoch 2 step 1300/3491 loss 0.0000\nEpoch 2 step 1350/3491 loss 0.0000\nEpoch 2 step 1400/3491 loss 0.0000\nEpoch 2 step 1450/3491 loss 0.0000\nEpoch 2 step 1500/3491 loss 0.0000\nEpoch 2 step 1550/3491 loss 0.0000\nEpoch 2 step 1600/3491 loss 0.0000\nEpoch 2 step 1650/3491 loss 0.0000\nEpoch 2 step 1700/3491 loss 0.0000\nEpoch 2 step 1750/3491 loss 0.0000\nEpoch 2 step 1800/3491 loss 0.0000\nEpoch 2 step 1850/3491 loss 0.0000\nEpoch 2 step 1900/3491 loss 0.0000\nEpoch 2 step 1950/3491 loss 0.0000\nEpoch 2 step 2000/3491 loss 0.0000\nEpoch 2 step 2050/3491 loss 0.0000\nEpoch 2 step 2100/3491 loss 0.0000\nEpoch 2 step 2150/3491 loss 0.0000\nEpoch 2 step 2200/3491 loss 0.0000\nEpoch 2 step 2250/3491 loss 0.0000\nEpoch 2 step 2300/3491 loss 0.0000\nEpoch 2 step 2350/3491 loss 0.0000\nEpoch 2 step 2400/3491 loss 0.0000\nEpoch 2 step 2450/3491 loss 0.0000\nEpoch 2 step 2500/3491 loss 0.0000\nEpoch 2 step 2550/3491 loss 0.0000\nEpoch 2 step 2600/3491 loss 0.0000\nEpoch 2 step 2650/3491 loss 0.0000\nEpoch 2 step 2700/3491 loss 0.0000\nEpoch 2 step 2750/3491 loss 0.0000\nEpoch 2 step 2800/3491 loss 0.0000\nEpoch 2 step 2850/3491 loss 0.0000\nEpoch 2 step 2900/3491 loss 0.0000\nEpoch 2 step 2950/3491 loss 0.0000\nEpoch 2 step 3000/3491 loss 0.0000\nEpoch 2 step 3050/3491 loss 0.0000\nEpoch 2 step 3100/3491 loss 0.0000\nEpoch 2 step 3150/3491 loss 0.0000\nEpoch 2 step 3200/3491 loss 0.0000\nEpoch 2 step 3250/3491 loss 0.0000\nEpoch 2 step 3300/3491 loss 0.0000\nEpoch 2 step 3350/3491 loss 0.0000\nEpoch 2 step 3400/3491 loss 0.0000\nEpoch 2 step 3450/3491 loss 0.0000\nEpoch 2 finished. avg loss 0.0000. skipped_batches 0\nSample train-batch loss: 0.0\nRunning validation health-check...\nValidation mean AUROC: 0.5054750843897461\nLabel 00 No Finding           pos=  199 zero=    0 uncertain=    0  AUC=0.5217174155061817\nLabel 01 Enlarged Cardiomedia pos=  106 zero=  197 uncertain=  121  AUC=0.49054860433145386\nLabel 02 Cardiomegaly         pos=  241 zero=  107 uncertain=   75  AUC=0.4654415112320986\nLabel 03 Lung Opacity         pos=  948 zero=   55 uncertain=   61  AUC=0.4833615095217468\nLabel 04 Lung Lesion          pos=   70 zero=   14 uncertain=   17  AUC=0.5863286454478165\nLabel 05 Edema                pos=  477 zero=  187 uncertain=  106  AUC=0.4903630014136834\nLabel 06 Consolidation        pos=  143 zero=  255 uncertain=  218  AUC=0.5046092840923212\nLabel 07 Pneumonia            pos=   59 zero=   32 uncertain=  162  AUC=0.5649368227106419\nLabel 08 Atelectasis          pos=  307 zero=    6 uncertain=  284  AUC=0.5323529921058353\nLabel 09 Pneumothorax         pos=  191 zero=  504 uncertain=   22  AUC=0.49182968230401225\nLabel 10 Pleural Effusion     pos=  752 zero=  310 uncertain=   89  AUC=0.4735630199809056\nLabel 11 Pleural Other        pos=   28 zero=    1 uncertain=   24  AUC=0.4971747319617502\nLabel 12 Fracture             pos=   93 zero=   23 uncertain=    3  AUC=0.48353829411731536\nLabel 13 Support Devices      pos= 1016 zero=   60 uncertain=   11  AUC=0.49088566673068307\nExemplar buffer size: 1200\nComputing Fisher (EWC)...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/99983984.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;31m# Save final artifacts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAVE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"training_metrics_final.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"model_state\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"NUM_LABELS\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNUM_LABELS\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAVE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"final_cl_model.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saved final artifacts to:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSAVE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdefault_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_iterable_to_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"index must be specified when data is not list-like\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_convert_platform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msubarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m                 \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mmaybe_convert_platform\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_dtype_obj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_convert_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.maybe_convert_objects\u001b[0;34m()\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Cannot convert numpy.ndarray to numpy.ndarray"],"ename":"TypeError","evalue":"Cannot convert numpy.ndarray to numpy.ndarray","output_type":"error"}],"execution_count":33},{"cell_type":"code","source":"# FINAL SAVE CELL — run now to persist results (model, optimizer, metrics, exemplar, fisher)\nimport os, json, torch, collections, numbers\n\nSAVE_DIR = \"/kaggle/working/outputs\"\nos.makedirs(SAVE_DIR, exist_ok=True)\n\n# 1) Save model weights (torch)\nmodel_path = os.path.join(SAVE_DIR, \"model_state_final.pth\")\ntorch.save({\"model_state\": model.state_dict(), \"NUM_LABELS\": NUM_LABELS}, model_path)\nprint(\"Saved model weights ->\", model_path)\n\n# 2) Save optimizer state if available\ntry:\n    opt_path = os.path.join(SAVE_DIR, \"optimizer_state_final.pth\")\n    torch.save(optimizer.state_dict(), opt_path)\n    print(\"Saved optimizer state ->\", opt_path)\nexcept Exception as e:\n    print(\"Optimizer not saved (missing or error):\", e)\n\n# 3) Save exemplar buffer (paths + labels) to JSON (labels -> lists of floats)\nexemplar_path = os.path.join(SAVE_DIR, \"exemplar_buffer.json\")\ntry:\n    exemplar_data = {\"paths\": exemplar_buffer.paths, \"labels\": []}\n    for lab in exemplar_buffer.labels:\n        # lab may be list/np/torch — convert to python list of floats\n        if isinstance(lab, torch.Tensor):\n            exemplar_data[\"labels\"].append(lab.detach().cpu().tolist())\n        elif isinstance(lab, (list, tuple)):\n            exemplar_data[\"labels\"].append([float(x) for x in lab])\n        elif hasattr(lab, \"tolist\"):\n            exemplar_data[\"labels\"].append([float(x) for x in lab.tolist()])\n        else:\n            exemplar_data[\"labels\"].append([float(lab)])\n    with open(exemplar_path, \"w\") as fw:\n        json.dump(exemplar_data, fw)\n    print(\"Saved exemplar buffer ->\", exemplar_path)\nexcept Exception as e:\n    print(\"Failed to save exemplar buffer:\", e)\n\n# 4) Save metrics as JSON (convert numpy types -> native)\nmetrics_path = os.path.join(SAVE_DIR, \"training_metrics_final.json\")\ndef convert_value(v):\n    if isinstance(v, (list, tuple)):\n        return [convert_value(x) for x in v]\n    if isinstance(v, dict):\n        return {str(k): convert_value(val) for k,val in v.items()}\n    if isinstance(v, torch.Tensor):\n        return convert_value(v.detach().cpu().tolist())\n    if hasattr(v, \"tolist\") and not isinstance(v, str):\n        try:\n            return convert_value(v.tolist())\n        except Exception:\n            pass\n    if isinstance(v, (numbers.Integral, numbers.Real)):\n        return v.item() if hasattr(v, \"item\") else v\n    return v\n\ntry:\n    safe_metrics = {k: convert_value(v) for k,v in metrics.items()}\n    with open(metrics_path, \"w\") as fw:\n        json.dump(safe_metrics, fw)\n    print(\"Saved training metrics ->\", metrics_path)\nexcept Exception as e:\n    print(\"Failed to save metrics via JSON fallback — saving with torch instead. Err:\", e)\n    torch.save(metrics, os.path.join(SAVE_DIR, \"training_metrics_final.pth\"))\n    print(\"Saved metrics (torch) ->\", os.path.join(SAVE_DIR, \"training_metrics_final.pth\"))\n\n# 5) Save fisher and old_params (EWC). Move tensors to CPU first (if exist)\newc_path = os.path.join(SAVE_DIR, \"ewc_fisher_oldparams.pth\")\ntry:\n    fisher_cpu = None\n    old_cpu = None\n    if 'fisher' in globals() and fisher is not None:\n        fisher_cpu = {int(k): (v.cpu() if isinstance(v, torch.Tensor) else v) for k,v in fisher.items()}\n    if 'old_params' in globals() and old_params is not None:\n        old_cpu = {int(k): (v.cpu() if isinstance(v, torch.Tensor) else v) for k,v in old_params.items()}\n    torch.save({\"fisher\": fisher_cpu, \"old_params\": old_cpu}, ewc_path)\n    print(\"Saved EWC fisher/old_params ->\", ewc_path)\nexcept Exception as e:\n    print(\"Failed to save fisher/old_params:\", e)\n\n# 6) Also save a convenience JSON summary (small) with key file locations and some training stats\nsummary = {\n    \"model_path\": os.path.basename(model_path),\n    \"optimizer_path\": os.path.basename(opt_path) if 'opt_path' in globals() and os.path.exists(opt_path) else None,\n    \"metrics_path\": os.path.basename(metrics_path),\n    \"exemplar_path\": os.path.basename(exemplar_path),\n    \"ewc_path\": os.path.basename(ewc_path) if os.path.exists(ewc_path) else None,\n    \"num_records\": sum(len(t) for t in task_recs_list) if 'task_recs_list' in globals() else None,\n    \"num_labels\": NUM_LABELS,\n    \"saved_at\": time.asctime()\n}\nwith open(os.path.join(SAVE_DIR, \"save_summary.json\"), \"w\") as fw:\n    json.dump(summary, fw)\nprint(\"Saved summary ->\", os.path.join(SAVE_DIR, \"save_summary.json\"))\n\nprint(\"\\nAll done. Files in\", SAVE_DIR)\nprint(os.listdir(SAVE_DIR))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T09:33:02.201208Z","iopub.execute_input":"2025-11-09T09:33:02.201557Z","iopub.status.idle":"2025-11-09T09:33:02.576923Z","shell.execute_reply.started":"2025-11-09T09:33:02.201535Z","shell.execute_reply":"2025-11-09T09:33:02.576322Z"}},"outputs":[{"name":"stdout","text":"Saved model weights -> /kaggle/working/outputs/model_state_final.pth\nSaved optimizer state -> /kaggle/working/outputs/optimizer_state_final.pth\nSaved exemplar buffer -> /kaggle/working/outputs/exemplar_buffer.json\nSaved training metrics -> /kaggle/working/outputs/training_metrics_final.json\nSaved EWC fisher/old_params -> /kaggle/working/outputs/ewc_fisher_oldparams.pth\nSaved summary -> /kaggle/working/outputs/save_summary.json\n\nAll done. Files in /kaggle/working/outputs\n['model_state_final.pth', 'exemplar_buffer.json', 'ewc_fisher_oldparams.pth', 'save_summary.json', 'training_metrics_final.json', 'optimizer_state_final.pth']\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# CELL: INFERENCE_AND_GRADCAM\n# Usage:\n# 1) Edit `MODEL_PATH` if different\n# 2) Call predict_and_gradcam([\"/kaggle/input/...?jpg\", ...], topk=5, do_gradcam=True)\n# Outputs saved to /kaggle/working/outputs/\n\nimport os, json, math, torch, numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as T\nimport torch.nn.functional as F\nfrom torchvision import models\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# ---- Config ----\nMODEL_PATH = \"/kaggle/working/outputs/model_state_final.pth\"   # adjust if needed\nOUTPUT_DIR = \"/kaggle/working/outputs\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nIMAGE_SIZE = 224\nTOPK = 5\n\n# canonical labels used in training\nLABELS = [\n    \"No Finding\",\"Enlarged Cardiomediastinum\",\"Cardiomegaly\",\"Lung Opacity\",\n    \"Lung Lesion\",\"Edema\",\"Consolidation\",\"Pneumonia\",\"Atelectasis\",\n    \"Pneumothorax\",\"Pleural Effusion\",\"Pleural Other\",\"Fracture\",\"Support Devices\"\n]\n\n# ---- model factory (must match training arch) ----\ndef get_densenet(num_labels=len(LABELS), pretrained=False):\n    # Try modern torchvision API, fallback to older one\n    try:\n        model = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT if pretrained else None)\n    except Exception:\n        model = models.densenet121(pretrained=pretrained)\n    in_f = model.classifier.in_features\n    model.classifier = torch.nn.Linear(in_f, num_labels)\n    return model\n\n# ---- load model weights ----\ndef load_model(model_path=MODEL_PATH, device=DEVICE):\n    assert os.path.exists(model_path), f\"Model file not found: {model_path}\"\n    ck = torch.load(model_path, map_location=\"cpu\")\n    num_labels = ck.get(\"NUM_LABELS\", len(LABELS))\n    model = get_densenet(num_labels=num_labels, pretrained=False)\n    model.load_state_dict(ck[\"model_state\"])\n    model = model.to(device).eval()\n    return model\n\n# ---- preprocessing (same normalization used in training) ----\ndef preprocess_pil(img_pil, image_size=IMAGE_SIZE):\n    transform = T.Compose([\n        T.Resize((image_size, image_size)),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485,0.485,0.485], std=[0.229,0.229,0.229])\n    ])\n    return transform(img_pil)  # tensor C,H,W\n\n# ---- helper: find last conv layer module in model (search modules in reverse) ----\ndef find_last_conv_module(model):\n    last_conv = None\n    for module in reversed(list(model.modules())):\n        if isinstance(module, torch.nn.Conv2d):\n            last_conv = module\n            break\n    return last_conv\n\n# ---- Grad-CAM implementation ----\ndef grad_cam(model, input_tensor, target_label_idx=None):\n    \"\"\"\n    input_tensor: single image tensor (C,H,W) on CPU or GPU\n    returns: heatmap numpy array (H,W) normalized 0..1\n    \"\"\"\n    model.eval()\n    device = next(model.parameters()).device\n    x = input_tensor.unsqueeze(0).to(device)  # 1,C,H,W\n\n    target_layer = find_last_conv_module(model)\n    if target_layer is None:\n        raise RuntimeError(\"No Conv2d layer found in model for Grad-CAM.\")\n\n    activations = []\n    gradients = []\n\n    def forward_hook(module, inp, out):\n        activations.append(out.detach())\n\n    def backward_hook(module, grad_in, grad_out):\n        # grad_out is a tuple\n        gradients.append(grad_out[0].detach())\n\n    fh = target_layer.register_forward_hook(forward_hook)\n    bh = target_layer.register_backward_hook(backward_hook)\n\n    out = model(x)  # logits\n    probs = torch.sigmoid(out).detach().cpu().numpy()[0]\n\n    if target_label_idx is None:\n        # choose highest scoring label\n        target_label_idx = int(np.argmax(probs))\n\n    # compute scalar loss for that label to backprop\n    score = out[0, target_label_idx]\n    model.zero_grad()\n    score.backward(retain_graph=False)\n\n    # get activation and gradient\n    if len(activations) == 0 or len(gradients) == 0:\n        fh.remove(); bh.remove()\n        raise RuntimeError(\"Grad-CAM hooks didn't capture activations/gradients.\")\n    act = activations[0].squeeze(0)     # C,Hf,Wf\n    grad = gradients[0].squeeze(0)      # C,Hf,Wf\n\n    # global average pooling of gradients -> weights\n    weights = torch.mean(grad, dim=(1,2))  # C\n    # weighted sum of activations\n    cam = torch.zeros(act.shape[1:], dtype=torch.float32, device=act.device)  # Hf,Wf\n    for i, w in enumerate(weights):\n        cam += w * act[i]\n    cam = torch.relu(cam)\n    cam = cam.cpu().numpy()\n    # normalize\n    cam = cam - cam.min()\n    if cam.max() != 0:\n        cam = cam / cam.max()\n    # resize to input image size\n    import cv2\n    cam_resized = cv2.resize(cam, (input_tensor.shape[2], input_tensor.shape[1]))\n    fh.remove(); bh.remove()\n    return cam_resized, probs, target_label_idx\n\n# ---- utility to overlay heatmap on PIL image ----\ndef overlay_heatmap_on_pil(pil_img, heatmap, alpha=0.5, colormap='jet'):\n    import matplotlib.cm as cm\n    hmap = (heatmap * 255).astype('uint8')\n    cmap = cm.get_cmap(colormap)\n    colored = cmap(hmap)[:, :, :3]  # H,W,3 float 0..1\n    colored = (colored * 255).astype('uint8')\n    colored_pil = Image.fromarray(colored).convert(\"RGBA\")\n    base = pil_img.resize(colored_pil.size).convert(\"RGBA\")\n    blended = Image.blend(base, colored_pil, alpha=alpha)\n    return blended\n\n# ---- prediction + gradcam main function ----\ndef predict_and_gradcam(image_paths, model=None, topk=TOPK, do_gradcam=True, save_outputs=True):\n    if model is None:\n        model = load_model()\n    results = []\n    for p in image_paths:\n        if not os.path.exists(p):\n            print(\"MISSING:\", p); continue\n        pil_im = Image.open(p).convert(\"RGB\")\n        tensor = preprocess_pil(pil_im)\n        # forward\n        with torch.no_grad():\n            out = model(tensor.unsqueeze(0).to(DEVICE))\n            probs = torch.sigmoid(out).cpu().numpy()[0]\n        # topk indices\n        topk_idx = np.argsort(probs)[::-1][:topk]\n        print(\"\\nImage:\", p)\n        for i, idx in enumerate(topk_idx):\n            print(f\" {i+1}. {LABELS[idx]}  prob={probs[idx]:.4f}\")\n        # produce grad-cam for top1 (or user-specified)\n        gradcam_path = None\n        if do_gradcam:\n            try:\n                cam, probs_all, chosen_idx = grad_cam(model, tensor, target_label_idx=None)\n                overlay = overlay_heatmap_on_pil(pil_im, cam, alpha=0.45)\n                base_name = os.path.splitext(os.path.basename(p))[0]\n                outname = os.path.join(OUTPUT_DIR, f\"gradcam_{base_name}_label{chosen_idx}.png\")\n                overlay.save(outname)\n                gradcam_path = outname\n                print(\" Saved Grad-CAM ->\", outname, f\"(label {chosen_idx} = {LABELS[chosen_idx]})\")\n            except Exception as e:\n                print(\" Grad-CAM failed for\", p, \":\", e)\n        results.append({\"path\": p, \"probs\": probs.tolist(), \"topk_idx\": topk_idx.tolist(), \"gradcam\": gradcam_path})\n    if save_outputs:\n        out_json = os.path.join(OUTPUT_DIR, \"inference_results.json\")\n        with open(out_json, \"w\") as fw:\n            json.dump(results, fw)\n        print(\"\\nSaved inference results JSON ->\", out_json)\n    return results\n\n# ---- Example usage: edit or pass your own list ----\n# imgs = [\"/kaggle/input/somewhere/img1.jpg\", \"/kaggle/input/somewhere/img2.jpg\"]\n# res = predict_and_gradcam(imgs, do_gradcam=True, topk=5)\n\nprint(\"Ready. Load your images and call predict_and_gradcam([...])\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T09:55:57.099933Z","iopub.execute_input":"2025-11-09T09:55:57.100618Z","iopub.status.idle":"2025-11-09T09:55:57.120520Z","shell.execute_reply.started":"2025-11-09T09:55:57.100595Z","shell.execute_reply":"2025-11-09T09:55:57.119757Z"}},"outputs":[{"name":"stdout","text":"Ready. Load your images and call predict_and_gradcam([...])\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"test_imgs = [\n    \"/kaggle/input/chexpert/valid/patient64541/study1/view1_frontal.jpg\",\n    \"/kaggle/input/chexpert/valid/patient64542/study1/view1_frontal.jpg\",\n    \"/kaggle/input/chexpert/train/patient00002/study1/view2_lateral.jpg\"\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T09:58:05.080078Z","iopub.execute_input":"2025-11-09T09:58:05.080578Z","iopub.status.idle":"2025-11-09T09:58:05.084142Z","shell.execute_reply.started":"2025-11-09T09:58:05.080555Z","shell.execute_reply":"2025-11-09T09:58:05.083386Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# FIXED inference + Grad-CAM: avoids torchvision transforms bug by using a safe PIL->tensor converter\nimport os, json, numpy as np, torch\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# CONFIG (keep same as before)\nMODEL_PATH = \"/kaggle/working/outputs/model_state_final.pth\"\nOUTPUT_DIR = \"/kaggle/working/outputs\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nIMAGE_SIZE = 224\nLABELS = [\n    \"No Finding\",\"Enlarged Cardiomediastinum\",\"Cardiomegaly\",\"Lung Opacity\",\n    \"Lung Lesion\",\"Edema\",\"Consolidation\",\"Pneumonia\",\"Atelectasis\",\n    \"Pneumothorax\",\"Pleural Effusion\",\"Pleural Other\",\"Fracture\",\"Support Devices\"\n]\n\n# Safe PIL -> tensor converter (same logic used during training)\ndef pil_to_tensor_safe_for_infer(pil_img, image_size=IMAGE_SIZE):\n    pil_img = pil_img.resize((image_size, image_size))\n    arr = np.asarray(pil_img)\n    if arr.ndim == 2:\n        arr = np.stack([arr]*3, axis=-1)\n    arr = arr.astype(np.float32) / 255.0\n    mean = np.array([0.485,0.485,0.485], dtype=np.float32)\n    std  = np.array([0.229,0.229,0.229], dtype=np.float32)\n    arr = (arr - mean) / std\n    arr = np.transpose(arr, (2,0,1)).copy()  # C,H,W contiguous\n    return torch.tensor(arr, dtype=torch.float32)\n\n# lightweight model loader (same architecture used for training)\nfrom torchvision import models\ndef get_densenet(num_labels=len(LABELS), pretrained=False):\n    try:\n        model = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT if pretrained else None)\n    except Exception:\n        model = models.densenet121(pretrained=pretrained)\n    in_f = model.classifier.in_features\n    model.classifier = torch.nn.Linear(in_f, num_labels)\n    return model\n\ndef load_model_safe(model_path=MODEL_PATH, device=DEVICE):\n    assert os.path.exists(model_path), f\"Model file not found: {model_path}\"\n    ck = torch.load(model_path, map_location=\"cpu\")\n    num_labels = ck.get(\"NUM_LABELS\", len(LABELS))\n    model = get_densenet(num_labels=num_labels, pretrained=False)\n    model.load_state_dict(ck[\"model_state\"])\n    model = model.to(device).eval()\n    return model\n\n# re-use grad_cam and helpers from previous cell (safe)\ndef find_last_conv_module(model):\n    last_conv = None\n    for module in reversed(list(model.modules())):\n        if isinstance(module, torch.nn.Conv2d):\n            last_conv = module\n            break\n    return last_conv\n\ndef grad_cam(model, input_tensor, target_label_idx=None):\n    model.eval()\n    device = next(model.parameters()).device\n    x = input_tensor.unsqueeze(0).to(device)  # 1,C,H,W\n\n    target_layer = find_last_conv_module(model)\n    if target_layer is None:\n        raise RuntimeError(\"No Conv2d layer found in model for Grad-CAM.\")\n\n    activations = []\n    gradients = []\n\n    def forward_hook(module, inp, out):\n        activations.append(out.detach())\n    def backward_hook(module, grad_in, grad_out):\n        gradients.append(grad_out[0].detach())\n\n    fh = target_layer.register_forward_hook(forward_hook)\n    bh = target_layer.register_backward_hook(backward_hook)\n\n    out = model(x)  # logits\n    probs = torch.sigmoid(out).detach().cpu().numpy()[0]\n\n    if target_label_idx is None:\n        target_label_idx = int(np.argmax(probs))\n\n    score = out[0, target_label_idx]\n    model.zero_grad()\n    score.backward(retain_graph=False)\n\n    if len(activations) == 0 or len(gradients) == 0:\n        fh.remove(); bh.remove()\n        raise RuntimeError(\"Grad-CAM hooks didn't capture activations/gradients.\")\n    act = activations[0].squeeze(0)\n    grad = gradients[0].squeeze(0)\n\n    weights = torch.mean(grad, dim=(1,2))\n    cam = torch.zeros(act.shape[1:], dtype=torch.float32, device=act.device)\n    for i, w in enumerate(weights):\n        cam += w * act[i]\n    cam = torch.relu(cam)\n    cam = cam.cpu().numpy()\n    cam = cam - cam.min()\n    if cam.max() != 0: cam = cam / cam.max()\n    import cv2\n    cam_resized = cv2.resize(cam, (input_tensor.shape[2], input_tensor.shape[1]))\n    fh.remove(); bh.remove()\n    return cam_resized, probs, target_label_idx\n\nfrom PIL import Image\ndef overlay_heatmap_on_pil(pil_img, heatmap, alpha=0.45, colormap='jet'):\n    import matplotlib.cm as cm\n    hmap = (heatmap * 255).astype('uint8')\n    cmap = cm.get_cmap(colormap)\n    colored = cmap(hmap)[:, :, :3]\n    colored = (colored * 255).astype('uint8')\n    colored_pil = Image.fromarray(colored).convert(\"RGBA\")\n    base = pil_img.resize(colored_pil.size).convert(\"RGBA\")\n    blended = Image.blend(base, colored_pil, alpha=alpha)\n    return blended\n\n# Safe predict + gradcam using the safe PIL->tensor conversion\ndef predict_and_gradcam_safe(image_paths, model=None, topk=5, do_gradcam=True, save_outputs=True):\n    if model is None:\n        model = load_model_safe()\n    results = []\n    for p in image_paths:\n        if not os.path.exists(p):\n            print(\"MISSING:\", p); continue\n        pil_im = Image.open(p).convert(\"RGB\")\n        tensor = pil_to_tensor_safe_for_infer(pil_im)  # C,H,W tensor\n        # forward\n        with torch.no_grad():\n            out = model(tensor.unsqueeze(0).to(DEVICE))\n            probs = torch.sigmoid(out).cpu().numpy()[0]\n        topk_idx = np.argsort(probs)[::-1][:topk]\n        print(\"\\nImage:\", p)\n        for i, idx in enumerate(topk_idx):\n            print(f\" {i+1}. {LABELS[idx]}  prob={probs[idx]:.4f}\")\n        gradcam_path = None\n        if do_gradcam:\n            try:\n                cam, probs_all, chosen_idx = grad_cam(model, tensor, target_label_idx=None)\n                overlay = overlay_heatmap_on_pil(pil_im, cam, alpha=0.45)\n                base_name = os.path.splitext(os.path.basename(p))[0]\n                outname = os.path.join(OUTPUT_DIR, f\"gradcam_{base_name}_label{chosen_idx}.png\")\n                overlay.save(outname)\n                gradcam_path = outname\n                print(\" Saved Grad-CAM ->\", outname, f\"(label {chosen_idx} = {LABELS[chosen_idx]})\")\n            except Exception as e:\n                print(\" Grad-CAM failed for\", p, \":\", e)\n        results.append({\"path\": p, \"probs\": probs.tolist(), \"topk_idx\": topk_idx.tolist(), \"gradcam\": gradcam_path})\n    if save_outputs:\n        out_json = os.path.join(OUTPUT_DIR, \"inference_results_safe.json\")\n        with open(out_json, \"w\") as fw:\n            json.dump(results, fw)\n        print(\"\\nSaved inference results JSON ->\", out_json)\n    return results\n\n# -------------------------\n# EDIT this list to your test images (example from CheXpert)\ntest_imgs = [\n    \"/kaggle/input/chexpert/valid/patient64541/study1/view1_frontal.jpg\",\n    \"/kaggle/input/chexpert/valid/patient64542/study1/view1_frontal.jpg\"\n]\n\n# Run it\nmodel = load_model_safe(MODEL_PATH, DEVICE)\nresults = predict_and_gradcam_safe(test_imgs, model=model, topk=5, do_gradcam=True)\nprint(\"Done. Results:\", results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T10:00:27.924508Z","iopub.execute_input":"2025-11-09T10:00:27.924774Z","iopub.status.idle":"2025-11-09T10:00:29.853403Z","shell.execute_reply.started":"2025-11-09T10:00:27.924754Z","shell.execute_reply":"2025-11-09T10:00:29.852656Z"}},"outputs":[{"name":"stdout","text":"\nImage: /kaggle/input/chexpert/valid/patient64541/study1/view1_frontal.jpg\n 1. Lung Opacity  prob=0.0000\n 2. Lung Lesion  prob=0.0000\n 3. Consolidation  prob=0.0000\n 4. No Finding  prob=0.0000\n 5. Cardiomegaly  prob=0.0000\n Saved Grad-CAM -> /kaggle/working/outputs/gradcam_view1_frontal_label3.png (label 3 = Lung Opacity)\n\nImage: /kaggle/input/chexpert/valid/patient64542/study1/view1_frontal.jpg\n 1. Lung Opacity  prob=0.0000\n 2. Atelectasis  prob=0.0000\n 3. Pneumonia  prob=0.0000\n 4. Enlarged Cardiomediastinum  prob=0.0000\n 5. Fracture  prob=0.0000\n Saved Grad-CAM -> /kaggle/working/outputs/gradcam_view1_frontal_label3.png (label 3 = Lung Opacity)\n\nSaved inference results JSON -> /kaggle/working/outputs/inference_results_safe.json\nDone. Results: [{'path': '/kaggle/input/chexpert/valid/patient64541/study1/view1_frontal.jpg', 'probs': [3.504835033307785e-10, 3.396322112436678e-10, 3.4993039021991024e-10, 3.6724681629074496e-10, 3.6390751523285303e-10, 3.4202973786534585e-10, 3.540086834785683e-10, 3.4169714280274377e-10, 3.1553273882600763e-10, 3.3950009470373743e-10, 3.3651836872650165e-10, 3.317728591856195e-10, 3.2683111772513485e-10, 3.196268527627666e-10], 'topk_idx': [3, 4, 6, 0, 2], 'gradcam': '/kaggle/working/outputs/gradcam_view1_frontal_label3.png'}, {'path': '/kaggle/input/chexpert/valid/patient64542/study1/view1_frontal.jpg', 'probs': [3.9735362222792503e-10, 4.224711691591665e-10, 4.055595304031101e-10, 4.422328614417381e-10, 4.0499523179526875e-10, 4.0669281831107185e-10, 3.933210146467303e-10, 4.2540557187997763e-10, 4.2713982351116897e-10, 4.050423607626641e-10, 3.87387871780831e-10, 3.9523417871834e-10, 4.1629361069439597e-10, 3.998160413853924e-10], 'topk_idx': [3, 8, 7, 1, 12], 'gradcam': '/kaggle/working/outputs/gradcam_view1_frontal_label3.png'}]\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"# SAFE incremental-runner: only runs if new data exists; otherwise does nothing.\nimport os, glob, csv, random, torch, numpy as np\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models\nimport torch.nn.functional as F\nfrom torch.cuda.amp import autocast, GradScaler\n\n# ---------- CONFIG ----------\nNEW_BATCH_DIR = \"/kaggle/input/new_chexpert_batch\"   # directory to drop new data\nLABELS_CSV_NAME = \"labels.csv\"                       # expected CSV inside NEW_BATCH_DIR\nBASE_MODEL_PATH = \"/kaggle/working/outputs/model_state_final.pth\"  # your saved model\nSAVE_INCREMENTAL = \"/kaggle/working/outputs/model_state_incremental.pth\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nIMAGE_SIZE = 224\nBATCH_SIZE = 8\nNUM_WORKERS = 0\nNUM_EPOCHS_INCREMENTAL = 1\nLEARNING_RATE = 1e-4\n\nLABELS = [\n    \"No Finding\",\"Enlarged Cardiomediastinum\",\"Cardiomegaly\",\"Lung Opacity\",\n    \"Lung Lesion\",\"Edema\",\"Consolidation\",\"Pneumonia\",\"Atelectasis\",\n    \"Pneumothorax\",\"Pleural Effusion\",\"Pleural Other\",\"Fracture\",\"Support Devices\"\n]\nNUM_LABELS = len(LABELS)\n\n# ---------- Helper: safe PIL -> tensor (same as training) ----------\ndef pil_to_tensor_safe(pil_img, image_size=IMAGE_SIZE):\n    pil_img = pil_img.resize((image_size, image_size))\n    arr = np.asarray(pil_img)\n    if arr.ndim == 2:\n        arr = np.stack([arr]*3, axis=-1)\n    arr = arr.astype(np.float32) / 255.0\n    mean = np.array([0.485,0.485,0.485], dtype=np.float32)\n    std  = np.array([0.229,0.229,0.229], dtype=np.float32)\n    arr = (arr - mean) / std\n    arr = np.transpose(arr, (2,0,1)).copy()\n    return torch.tensor(arr, dtype=torch.float32)\n\n# ---------- Check for new data existence ----------\nif not os.path.isdir(NEW_BATCH_DIR):\n    print(f\"No incremental folder found at `{NEW_BATCH_DIR}` — nothing to do. (Drop new data there to trigger incremental training.)\")\nelse:\n    # find image files\n    img_files = sorted(glob.glob(os.path.join(NEW_BATCH_DIR, \"**\", \"*.jpg\"), recursive=True) +\n                       glob.glob(os.path.join(NEW_BATCH_DIR, \"**\", \"*.png\"), recursive=True))\n    labels_csv_path = os.path.join(NEW_BATCH_DIR, LABELS_CSV_NAME)\n    if len(img_files) == 0 and not os.path.exists(labels_csv_path):\n        print(f\"Found directory `{NEW_BATCH_DIR}` but no images or `{LABELS_CSV_NAME}` inside — nothing to do.\")\n    elif not os.path.exists(labels_csv_path):\n        print(f\"Found {len(img_files)} image(s) in `{NEW_BATCH_DIR}` but missing `{LABELS_CSV_NAME}`.\")\n        print(\"Please provide a labels.csv with a 'resolved_path' column (absolute paths) and the 14 label columns.\")\n        print(\"Example CSV header: resolved_path,No Finding,Enlarged Cardiomediastinum,Cardiomegaly,...,Support Devices\")\n    else:\n        # read CSV rows; expect resolved_path column that points to image files (absolute or relative inside NEW_BATCH_DIR)\n        print(\"Detected new dataset folder and labels.csv — preparing incremental update.\")\n        rows = []\n        with open(labels_csv_path, 'r', newline='') as fr:\n            rdr = csv.DictReader(fr)\n            for r in rdr:\n                p = r.get(\"resolved_path\") or r.get(\"Path\") or r.get(\"path\")\n                if not p:\n                    continue\n                # if path is relative, make it relative to NEW_BATCH_DIR\n                if not os.path.isabs(p):\n                    p = os.path.join(NEW_BATCH_DIR, p)\n                if not os.path.exists(p):\n                    # skip missing image\n                    print(\"Skipping missing image:\", p)\n                    continue\n                # build label vector of length NUM_LABELS\n                lab = []\n                for c in LABELS:\n                    v = r.get(c, \"\")\n                    try:\n                        f = float(v) if v not in (None,\"\") else 0.0\n                    except:\n                        f = 0.0\n                    # treat -1 uncertain as 0 for incremental fine-tune; modify if you want to mask instead\n                    if f == -1.0: f = 0.0\n                    lab.append(float(f))\n                rows.append((p, lab))\n        if len(rows) == 0:\n            print(\"No valid labeled images found in CSV — nothing to do.\")\n        else:\n            print(f\"Found {len(rows)} labeled images — starting incremental training (epochs={NUM_EPOCHS_INCREMENTAL}).\")\n\n            # ---------- Minimal dataset + dataloader ----------\n            class NewCXDataset(Dataset):\n                def __init__(self, entries):\n                    self.entries = entries\n                def __len__(self): return len(self.entries)\n                def __getitem__(self, idx):\n                    p, lab = self.entries[idx]\n                    im = Image.open(p).convert(\"RGB\")\n                    t = pil_to_tensor_safe(im, image_size=IMAGE_SIZE)\n                    return t, torch.tensor(lab, dtype=torch.float32)\n\n            dataset = NewCXDataset(rows)\n            loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n\n            # ---------- Load base model ----------\n            assert os.path.exists(BASE_MODEL_PATH), f\"Base model not found at {BASE_MODEL_PATH}\"\n            ck = torch.load(BASE_MODEL_PATH, map_location=\"cpu\")\n            # build densenet\n            try:\n                model = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n            except Exception:\n                model = models.densenet121(pretrained=True)\n            in_f = model.classifier.in_features\n            model.classifier = torch.nn.Linear(in_f, NUM_LABELS)\n            model.load_state_dict(ck[\"model_state\"])\n            model = model.to(DEVICE).train()\n\n            optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n            scaler = GradScaler()\n\n            # ---------- Small training loop ----------\n            for epoch in range(NUM_EPOCHS_INCREMENTAL):\n                running_loss = 0.0\n                for i, (imgs, labs) in enumerate(loader):\n                    imgs = imgs.to(DEVICE); labs = labs.to(DEVICE)\n                    optimizer.zero_grad()\n                    with autocast():\n                        out = model(imgs)\n                        loss = F.binary_cross_entropy_with_logits(out, labs)\n                    scaler.scale(loss).backward()\n                    scaler.step(optimizer)\n                    scaler.update()\n                    running_loss += float(loss.item())\n                    if (i+1) % 10 == 0 or (i+1) == len(loader):\n                        print(f\"Epoch {epoch+1} step {i+1}/{len(loader)} loss={running_loss/(i+1):.4f}\")\n\n            # ---------- Save new incremental model ----------\n            torch.save({\"model_state\": model.state_dict(), \"NUM_LABELS\": NUM_LABELS}, SAVE_INCREMENTAL)\n            print(\"Incremental update complete. Saved to:\", SAVE_INCREMENTAL)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T10:11:53.268065Z","iopub.execute_input":"2025-11-09T10:11:53.268369Z","iopub.status.idle":"2025-11-09T10:11:53.286836Z","shell.execute_reply.started":"2025-11-09T10:11:53.268346Z","shell.execute_reply":"2025-11-09T10:11:53.286129Z"}},"outputs":[{"name":"stdout","text":"No incremental folder found at `/kaggle/input/new_chexpert_batch` — nothing to do. (Drop new data there to trigger incremental training.)\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"# CELL: VISUALIZE_TRAINING_METRICS\nimport os, json, math, numpy as np, matplotlib.pyplot as plt\nfrom matplotlib import ticker\nOUTPUT_DIR = \"/kaggle/working/outputs\"\nMETRICS_JSON = os.path.join(OUTPUT_DIR, \"training_metrics_final.json\")\nEXEMPLAR_JSON = os.path.join(OUTPUT_DIR, \"exemplar_buffer.json\")\nPLOT_DIR = os.path.join(OUTPUT_DIR, \"metrics_plots\")\nos.makedirs(PLOT_DIR, exist_ok=True)\n\nassert os.path.exists(METRICS_JSON), f\"Metrics file not found: {METRICS_JSON}\"\nwith open(METRICS_JSON,'r') as fr:\n    metrics = json.load(fr)\n\n# normalize metrics keys/numpy -> python\ndef to_list(x):\n    if x is None: return []\n    if isinstance(x, list): return x\n    try:\n        return list(x)\n    except:\n        return [x]\n\ntask = to_list(metrics.get(\"task\", []))\nepoch = to_list(metrics.get(\"epoch\", []))\navg_loss = to_list(metrics.get(\"avg_loss\", []))\nmean_auc = to_list(metrics.get(\"mean_auc\", []))\nskipped = to_list(metrics.get(\"skipped_batches\", []))\n\n# plot 1: avg_loss vs global step (or epoch index)\nplt.figure(figsize=(8,4))\nif len(avg_loss) == 0:\n    print(\"No avg_loss data found in metrics.\")\nelse:\n    x = np.arange(1, len(avg_loss)+1)\n    plt.plot(x, np.array(avg_loss, dtype=float), marker='o', linewidth=1)\n    plt.title(\"Avg training loss (per recorded epoch)\")\n    plt.xlabel(\"record (epoch entry)\")\n    plt.ylabel(\"avg loss\")\n    plt.grid(alpha=0.3)\n    fpath = os.path.join(PLOT_DIR, \"avg_loss.png\")\n    plt.savefig(fpath, bbox_inches='tight', dpi=150)\n    plt.show()\n    print(\"Saved:\", fpath)\n\n# plot 2: mean AUROC vs recorded epoch\nplt.figure(figsize=(8,4))\nif len(mean_auc) == 0:\n    print(\"No mean_auc data found in metrics.\")\nelse:\n    x = np.arange(1, len(mean_auc)+1)\n    ma = np.array([float('nan') if (v is None) else float(v) for v in mean_auc], dtype=float)\n    plt.plot(x, ma, marker='o', linewidth=1, color='tab:orange')\n    plt.title(\"Mean AUROC (validation) per epoch/task checkpoint\")\n    plt.xlabel(\"record (epoch entry)\")\n    plt.ylabel(\"mean AUROC\")\n    plt.ylim(0.0,1.0)\n    plt.grid(alpha=0.3)\n    fpath = os.path.join(PLOT_DIR, \"mean_auc.png\")\n    plt.savefig(fpath, bbox_inches='tight', dpi=150)\n    plt.show()\n    print(\"Saved:\", fpath)\n\n# If per-label AUC history is available (some setups include per-label arrays) attempt to plot final per-label AUC\n# Try to read per-label values if present in metrics (e.g., metrics['label_aucs'] or metrics['aucs'])\nlabel_aucs = None\n# common shapes: metrics may store last epoch's per-label AUCs under 'aucs' or similar\nif \"aucs\" in metrics:\n    label_aucs = metrics[\"aucs\"]\n# fallback: inspect last entry if it's a dict of per-label\nif label_aucs is None and isinstance(mean_auc, list) and len(mean_auc)>0:\n    # try reading a per-epoch stored container: metrics may contain 'per_label' key\n    for k in metrics:\n        if isinstance(metrics[k], list) and len(metrics[k])==len(mean_auc):\n            # skip avg_loss/mean_auc etc.\n            pass\n\n# Try to load a saved metrics JSON that may contain per-label arrays saved separately:\n# We'll attempt to load 'inference_results' or 'per_label_aucs.json' nearby\nper_label_path = os.path.join(OUTPUT_DIR, \"per_label_aucs.json\")\nif os.path.exists(per_label_path):\n    with open(per_label_path,'r') as fr:\n        label_aucs = json.load(fr)\n\n# If still None, try to reconstruct final label AUCs from training_metrics_final.json if stored as list under 'final_label_aucs'\nif label_aucs is None and \"final_label_aucs\" in metrics:\n    label_aucs = metrics[\"final_label_aucs\"]\n\n# If still none, we will attempt to compute per-label AUC from validation results file if exists:\nif label_aucs is None:\n    # try to read inference_results.json saved earlier (may only be predictions)\n    irp = os.path.join(OUTPUT_DIR, \"inference_results.json\")\n    if os.path.exists(irp):\n        print(\"Found inference_results.json — but it contains predictions, not per-label AUCs.\")\n    else:\n        print(\"Per-label AUCs not found in metrics. If you want per-label AUCs, run safe_evaluate_auroc on val_loader and save per-label AUCs to per_label_aucs.json.\")\n\n# If we have label_aucs (list/dict) — plot bar chart\nif label_aucs is not None:\n    if isinstance(label_aucs, dict):\n        labels = list(label_aucs.keys())\n        vals = [float(label_aucs[k]) if label_aucs[k] is not None else float('nan') for k in labels]\n    else:\n        # assume list\n        try:\n            vals = [float(x) if x is not None else float('nan') for x in label_aucs]\n            labels = [f\"Label {i}\" for i in range(len(vals))]\n        except Exception:\n            labels = [f\"Label {i}\" for i in range(len(label_aucs))]\n            vals = [float('nan')] * len(labels)\n    # plot bar\n    plt.figure(figsize=(10,4))\n    x = np.arange(len(labels))\n    plt.bar(x, vals)\n    plt.xticks(x, labels, rotation=70)\n    plt.ylim(0.0, 1.0)\n    plt.title(\"Per-label AUC (final)\")\n    plt.ylabel(\"AUC\")\n    plt.grid(alpha=0.2, axis='y')\n    fpath = os.path.join(PLOT_DIR, \"per_label_auc.png\")\n    plt.savefig(fpath, bbox_inches='tight', dpi=150)\n    plt.show()\n    print(\"Saved:\", fpath)\nelse:\n    print(\"No per-label AUC data available to plot. You can compute per-label AUCs using `safe_evaluate_auroc(model, val_loader, ...)` and save as per_label_aucs.json in outputs/.\")\n\n# Optional: examine exemplar buffer size if present\nif os.path.exists(EXEMPLAR_JSON):\n    with open(EXEMPLAR_JSON,'r') as fr:\n        ex = json.load(fr)\n    n_ex = len(ex.get(\"paths\", []))\n    print(f\"Exemplar buffer size (from exemplar_buffer.json): {n_ex}\")\nelse:\n    print(\"No exemplar_buffer.json found in outputs/.\")\n\n# Print location of saved plots\nprint(\"\\nPlots saved to:\", PLOT_DIR)\nprint(os.listdir(PLOT_DIR))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T10:13:19.042391Z","iopub.execute_input":"2025-11-09T10:13:19.043061Z","iopub.status.idle":"2025-11-09T10:13:19.778238Z","shell.execute_reply.started":"2025-11-09T10:13:19.043035Z","shell.execute_reply":"2025-11-09T10:13:19.777517Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x400 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"Saved: /kaggle/working/outputs/metrics_plots/avg_loss.png\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x400 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"Saved: /kaggle/working/outputs/metrics_plots/mean_auc.png\nPer-label AUCs not found in metrics. If you want per-label AUCs, run safe_evaluate_auroc on val_loader and save per-label AUCs to per_label_aucs.json.\nNo per-label AUC data available to plot. You can compute per-label AUCs using `safe_evaluate_auroc(model, val_loader, ...)` and save as per_label_aucs.json in outputs/.\nExemplar buffer size (from exemplar_buffer.json): 1200\n\nPlots saved to: /kaggle/working/outputs/metrics_plots\n['mean_auc.png', 'avg_loss.png']\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"# CELL: EXPORT_TORCHSCRIPT_AND_ONNX\nimport os, torch, numpy as np\nfrom torchvision import models\nfrom PIL import Image\n\nMODEL_PTH = \"/kaggle/working/outputs/model_state_final.pth\"\nTS_OUT = \"/kaggle/working/outputs/model_torchscript.pt\"\nONNX_OUT = \"/kaggle/working/outputs/model.onnx\"\nIMAGE_SIZE = 224\nNUM_LABELS = 14  # change if different\n\nassert os.path.exists(MODEL_PTH), \"Model checkpoint not found.\"\n\n# 1) load state dict (same architecture as training)\ndef get_densenet(num_labels=NUM_LABELS):\n    try:\n        model = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n    except Exception:\n        model = models.densenet121(pretrained=True)\n    in_f = model.classifier.in_features\n    model.classifier = torch.nn.Linear(in_f, num_labels)\n    return model\n\nck = torch.load(MODEL_PTH, map_location=\"cpu\")\nmodel = get_densenet(num_labels=ck.get(\"NUM_LABELS\", NUM_LABELS))\nmodel.load_state_dict(ck[\"model_state\"])\nmodel.eval().cpu()\n\n# Create a dummy input (float tensor) for tracing/export\ndummy = torch.randn(1, 3, IMAGE_SIZE, IMAGE_SIZE, dtype=torch.float32)\n\n# TorchScript (trace) - fast and simple\ntry:\n    traced = torch.jit.trace(model, dummy, strict=False)\n    traced.save(TS_OUT)\n    print(\"Saved TorchScript ->\", TS_OUT)\nexcept Exception as e:\n    print(\"TorchScript trace failed:\", e)\n\n# ONNX export (dynamic batch, dynamic height/width optional)\ntry:\n    input_names = [\"input\"]\n    output_names = [\"output\"]\n    dynamic_axes = {\"input\": {0: \"batch_size\", 2: \"height\", 3: \"width\"}, \"output\": {0: \"batch_size\"}}\n    torch.onnx.export(model, dummy, ONNX_OUT,\n                      export_params=True, opset_version=13,\n                      input_names=input_names, output_names=output_names,\n                      dynamic_axes=dynamic_axes, verbose=False)\n    print(\"Saved ONNX ->\", ONNX_OUT)\nexcept Exception as e:\n    print(\"ONNX export failed:\", e)\n\n# Quick size info\nimport os\nfor f in [TS_OUT, ONNX_OUT]:\n    if os.path.exists(f):\n        print(f, \"size:\", round(os.path.getsize(f)/(1024*1024),2), \"MB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T10:17:09.136597Z","iopub.execute_input":"2025-11-09T10:17:09.137286Z","iopub.status.idle":"2025-11-09T10:17:14.398022Z","shell.execute_reply.started":"2025-11-09T10:17:09.137263Z","shell.execute_reply":"2025-11-09T10:17:14.397392Z"}},"outputs":[{"name":"stdout","text":"Saved TorchScript -> /kaggle/working/outputs/model_torchscript.pt\nSaved ONNX -> /kaggle/working/outputs/model.onnx\n/kaggle/working/outputs/model_torchscript.pt size: 27.92 MB\n/kaggle/working/outputs/model.onnx size: 26.96 MB\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"!pip install onnxruntime","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T10:19:12.862752Z","iopub.execute_input":"2025-11-09T10:19:12.863463Z","iopub.status.idle":"2025-11-09T10:19:19.181075Z","shell.execute_reply.started":"2025-11-09T10:19:12.863438Z","shell.execute_reply":"2025-11-09T10:19:19.180094Z"}},"outputs":[{"name":"stdout","text":"Collecting onnxruntime\n  Downloading onnxruntime-1.23.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\nCollecting coloredlogs (from onnxruntime)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\nRequirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (2.2.6)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (6.33.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\nDownloading onnxruntime-1.23.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\nSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.23.2\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"# CELL: RUN_ONNX_RUNTIME (requires onnxruntime installed: pip install onnxruntime)\nimport onnxruntime as ort\nimport numpy as np\nfrom PIL import Image\nimport os\n\nONNX_OUT = \"/kaggle/working/outputs/model.onnx\"\nIMAGE_SIZE = 224\nLABELS = [...]  # put your 14 labels list here or import from previous cell\n\ndef preprocess_pil(pil_img, image_size=IMAGE_SIZE):\n    pil_img = pil_img.resize((image_size, image_size))\n    arr = np.asarray(pil_img).astype(np.float32)/255.0\n    if arr.ndim == 2:\n        arr = np.stack([arr]*3, -1)\n    mean = np.array([0.485,0.485,0.485], dtype=np.float32)\n    std  = np.array([0.229,0.229,0.229], dtype=np.float32)\n    arr = (arr - mean) / std\n    arr = np.transpose(arr, (2,0,1))  # C,H,W\n    arr = np.expand_dims(arr, 0)      # 1,C,H,W\n    return arr.astype(np.float32)\n\nsess = ort.InferenceSession(ONNX_OUT)\n# example:\nimg_path = \"/kaggle/input/chexpert/valid/patient64541/study1/view1_frontal.jpg\"\nimg = Image.open(img_path).convert(\"RGB\")\ninp = preprocess_pil(img)\noutputs = sess.run(None, {\"input\": inp})\nprobs = 1.0 / (1.0 + np.exp(-outputs[0]))  # sigmoid on logits\nprint(\"Top preds:\", np.argsort(probs[0])[::-1][:5])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T10:20:10.612939Z","iopub.execute_input":"2025-11-09T10:20:10.613655Z","iopub.status.idle":"2025-11-09T10:20:10.780987Z","shell.execute_reply.started":"2025-11-09T10:20:10.613627Z","shell.execute_reply":"2025-11-09T10:20:10.780313Z"}},"outputs":[{"name":"stdout","text":"Top preds: [3 4 6 0 2]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}